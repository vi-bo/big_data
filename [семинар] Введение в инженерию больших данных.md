НИТУ "МИСиС"

# Введение в инженерию больших данных

https://openedu.ru/

## Практическое задание 0 (Cloudera Manager, PuTTy)

1. Устанавливаем VirtualBox + putty (для доступа на удаленную машину).

2. Скачиваем образ виртуальной машины.

3. Импортируем в VirtualBox образ.
   
   <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-09-28-49-image.png" alt="" data-align="inline">

4. Запускаем cloudera-quickstart-vm-5.13.0.0-virtualbox
   
   <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-29-20-55-51-image.png" alt="" data-align="inline">

5. После запуска автоматически открывается cloudera.

6. Видим какие созданы узлы и их ip-адреса.

7. Для выхода из VirtualBox по умолчанию `Right Ctrl`

8. Переходим по ссылке Cloudera Manager, вводим логин и пароль (cloudera/cloudera)
   
   <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-29-20-59-31-image.png" alt="" data-align="inline">

9. Видим наш кластер
   
   <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-29-21-03-08-image.png" alt="" data-align="inline">

10. Переходим на вкладку Hosts - All Hosts
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-09-57-01-image.png" alt="" data-align="inline">
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-09-58-23-image.png" alt="" data-align="inline">

11. Нужно запустить сервисы и проверить, что они работают. Можно запускать сервисы по отдельности, можно все сразу. В разделе Cloudera QuickStart нажимаем `Start`, чтобы запустить все сразу.
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-00-57-image.png" alt="" data-align="inline">
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-05-05-image.png" alt="" data-align="inline">

12. Также в разделе Cloudera Managment Servise запустим сервис для управления кластером.
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-11-23-image.png" alt="" data-align="inline">
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-12-16-image.png" alt="" data-align="inline">

13. Переходим в браузере на `http://localhost:7180`. Снова вводим логин и пароль (cloudera/cloudera)
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-20-29-image.png" alt="" data-align="inline">

14. **Отключаем AdblockerUltimate и прочее не странице веб-браузера**. Теперь можно работать в браузерной версии ClouderaManager.
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-09-45-01-image.png" alt="" data-align="inline">

15. Чтобы подключиться к виртуальной машине через PuTTy нужно пробросить свободный порт. Заходим в настройки виртуальной машины в virtualbox Network -> Advanced -> Port Forwarding. Добавляем:
    
    - Name (SSH)
    
    - Protocol (TCP)
    
    - Host Port (например, 2222)
    
    - Guest IP (10.0.2.15, получаем через ifconfig в терминале виртуальной машины)
    
    - Guest Port (22)
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-11-15-52-image.png" alt="" data-align="inline">
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-11-21-21-image.png" alt="" data-align="inline">

16. Теперь можно подключаться через PuTTY. Запускаем `localhost` на порту `2222`, вводим логин/пароль (cloudera/cloudera).
    
    <img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-11-32-24-image.png" alt="" data-align="inline">

17. Проверяем версию hadoop `hadoop version`.

18. В файле `hosts` добавляем строку `127.0.0.1 quickstart.cloudera` (локальный ip-адрес и имя).

19. Переходим в сервис Hue.

20. Переходим в графический интерфейс Hue (в адресе должен быть уже `quickstart.cloudera`). Вводим логин и пароль.

## Практическое задание 1 (HDFS, Hue)

1. В PuTTy набираем `hdfs`. Видим, что мы можем сделать с файловой системой.
   
   > `hdfs`  # список команд

2. Команда `hdfs dfs` - выводит список команд файловой системы HDFS.
   
   > `hdfs dfs` # список команд файловой системы

3. `hdfs dfs -help mkdir` - вывод справки по команде создания папки.
   
   > `hdfs dfs -help имя_команды` # вывод справки по команде

4. Создаем папку `hdfs dfs mkdir /user/cloudera/example_tweets`
   
   > `hdfs dfs mkdir имя_папки` # создание папки

5. Создаем вторую папку `hdfs dfs mkdir /user/cloudera/tweets`.

6. Меняем привилегии для второй папки. Выведем подсказку по команде `chmod` - `hdfs dfs -help chmod`. Разрешим всем писать в эту папку `hdfs dfs -chmod 777 /user/cloudera/tweets` (777 - разрешение).
   
   > `hdfs dfs -chmod 777 имя_папки` # изменение прав доступа к папке

7. Для просмотра используем команду `hdfs dfs -ls /user/cloudera`
   
   > `hdfs dfs -ls путь/имя_папки` # просмотр файлов дирректории

8. Аналогично можно посмотреть через браузер (сервис Hue).

9. Чтобы скопировать файл с локальной виртуальной машины используем команду  `hdfs dfs -put example.json /user/cloudera/example_tweets` (предварительно посмотреть размер файла можно командой `ls -lh`)
   
   > `ls -lh` # просмотр дирректории с правамми доступа, размером
   > 
   > `hdfs dfs -put имя_файла путь/имя_папки` # копирование файла

10. Аналогично можно проверить, что файл скопировался через Hue. На локальном компьютере просмотреть данный файл весьма проблематично (1 Gb), смотрим через Hue.

11. Создаем через Hue папку `test`. Добавляем файл, удаляем папку через web-интерфейс Hue.

## Практическое задание 2 (Wordcount)

1. Посчитаем количество слов в файле `example.json`.

2. Иcпользуем MapReduce, готовую реализацию подсчета слов из библиотеки hadoop examples. 

3. В корневой дирректории home/cloudera лежит файл hadoop-examples.jar, запустим его `hadoop jar hadoop-examples.jar`. Открывается перечень приложений.
   
   > `hadoop jar hadoop-examples.jar` # предустановленные приложения

4. Запускаем `hadoop jar hadoop-examples.jar wordcount /user/cloudera/example_tweets /user/cloudera/wordcount_result` (где лежит исходный файл и куда будем складывать результат). ВАЖНО! Папка назначения не должна существовать.
   
   > hadoop jar имя_архива_.jar имя_приложения путь/исходный_файл путь/итоговый_файл # запуск приложения

5. Файл делится на 8 splits, видим номер задачи (job). Также дается ссылка по которой можно контролировать выполнение job. Копируем ссылку в браузер.

6. Результат смотрим в папке `/user/cloudera/wordcount_result`. Здесь лежат два файла `_SECCESS` (метка, что процесс завершился успешно) и `part-r-00000` (результат 481,1 Mb). Буква r - говорит о том, что это результат reduce задачи (m - map).

7. Wordcount разбивает все по пробелам.

## Практическое задание 3 (Apache Flume)

1. Идем в Twitter, регистрируемся как разработчик, получаем ключи для создания приложения, которое будет получать потоковые данные из Twitter. Настроим Flume, чтобы он забирал информацию и раскладываел ее по файлам в HDFS. Приложение настроим на некую тематику по ключевым словам.

2. Идем на `apps.twitter.com` (apply for developer account), создаем приложение `bigdata_demo_application`. На вкладке `Keys and Access Tokens` для конфигурирования Flume получаем:
   
   - Consumer Key (API Key)
   
   - Consumer Secret (API Secret)
   
   - Access Token (нужно сгенерировать)
   
   - Access Token Secret (нужно сгенерировать)

3. Переходим к настройке Flume. В Clouder Manager переходим в сервис Flume, вкладка Configuration, раздел Configuration File и Agent Name.

4. Заполняем полк Agent Name - TwitterAgent.

5. Заполняем поле Configuration File (хотим вязть данные из Twitter и через оперативную память передать их в HDFS):
   
   ```textile
   # vibo: откуда поступают данные (Twitter)
   # Имя_нашего_агента.источники_данных
   TwitterAgent.sources = Twitter
   
   # vibo: что мы будем использовать в качестве канала (канал в оперативной памяти)
   # Имя_нашего_агента.каналы_передачи_данных
   TwitterAgent.channels = MemChannel
   
   # vibo: куда будем сливать наши данные (в HDFS)
   # Имя_нашего_агента.стоки
   TwitterAgent.sinks = HDFS
   ```

6. Продолжаем заполнять поле Configuration File - переходим к настройке источника данных. Настройка происходит следующим образом. Мы говорим название агента (TwitterAgent), далее говорим через точку источники (sources), название источника, который мя сейчас хотим настроить (Twitter), далее указываем:
   
   - .type = com.cloudera.flume.source.TwitterSourse (класс библиотеки, которая будет отвечать за сбор информации из Twitter; используем готовую библиотеку от Cloudera, импортированную на кластер). Если нужны другие источники - можно посмотреть на сайте Flume.
   
   - .channels = MemChannel (источник должен передавать данные в наш канал)
   
   - .consumerKey, .consumerSecret, .accessToken, .accessTokenSecret (получаем при регистрации приложения в режиме разработчика Twitter)
   
   - .keywords = russia, moscow, россия, москва (ключевые слова, которые мы хотим искать, Twitter будет пересылать сообщения, содержащие указанные слова)
   
   ```textile
   # Source Twitter
   
   # Имя_нашего_агента.источники_данных.Twitter.класс_отвечающий_за_источник
   TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSourse
   
   # Имя_нашего_агента.источники_данных.Twitter.название_канала
   TwitterAgent.sources.Twitter.channels = MemChannel
   
   TwitterAgent.sources.Twitter.consumerKey = xxxxxxxxxxxxxxxxxxxx
   TwitterAgent.sources.Twitter.consumerSecret = xxxxxxxxxxxxxxxxxxxx
   TwitterAgent.sources.Twitter.accessToken = xxxxxxxxxxxxxxxxxxxx
   TwitterAgent.sources.Twitter.accessTokenSecret = xxxxxxxxxxxxxxxxxxxx
   
   # Имя_нашего_агента.источники_данных.Twitter.ключевые_слова_по_нашей_теме_через_запятую
   TwitterAgent.sources.Twitter.keywords = 
   ```

7. Настраиваем сток (куда собираемся сливать данные). В нашем случае это будет HDFS. Указываем агент (TwitterAgent), через точку сток (sinks), имя стока (HDFS, имя задали в первом блоке)
   
   - .channel = MemChannel (канал откуда будем забирать данные)
   
   - .type = hdfs (указываем тип, hdfs - зарезераированное слово)
     
     - .path (путь куда сбрасываем данные)
     
     - .inUsePrefix =. (это значит, что файл, который только начал записываться имеет имя, начинающееся с точки, пока он не запишется)
     
     - .fileType = DataStream (тип файла, DataStream)
   
   ```textile
   # SINK HDFS
   
   # Имя_нашего_агента.стоки.HDFS.каналы_передачи_данных
   TwitterAgent.sinks.HDFS.channel = MemChannel
   
   # Имя_нашего_агента.стоки.HDFS.тип_стока
   TwitterAgent.sinks.HDFS.type = hdfs
   
   # Имя_нашего_агента.стоки.HDFS.путь_к_папке_назначения
   TwitterAgent.sinks.HDFS.hdfs.path = hdfs:///user/cloudera/tweets
   
   # Имя_нашего_агента.стоки.HDFS.префикс_для_временных_файлов
   TwitterAgent.sinks.HDFS.hdfs.inUsePrefix =.
   
   # Имя_нашего_агента.стоки.HDFS.формат_файла
   TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
   
   # Настройка частоты сброса и размер файла.
   TwitterAgent.sinks.HDFS.hdfs.batchSize = 10
   TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
   TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600
   TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
   ```

8. Подробнее о том, какие параметры можно настраивать при сборе данных можно посмотреть на сайте Flume. В т.ч. можно настраивать имена файлов - добавлять в них дату, время и т.д.

9. Последним этапом настраиваем канал (CHANNEL). Говорим, MemChannel (мы его так назвали в первом блоке) будет иметь тип memory, объем канала 100 сообщений.
   
   ```textile
   # CHANNEL
   
   # Имя_нашего_агента.каналы.MemChannel.тип_канала
   TwitterAgent.channels.MemChannel.type = memory
   
   # Имя_нашего_агента.каналы.MemChannel.объем канала
   TwitterAgent.channels.MemChannel.capacity = 100
   ```

10. Сохраняем. Первоначальная настройка Flume для сбора данных из Twitter завершена. Возвращаемся в основное меню Cloudera Manager. Видим дополнительный значек напротив Flume, который говорит, что конфигурация Flume устарела. Перезапускаем сервис с новой конфигурацией.

11. После перезапуска сервиса, Flume начал собирать данные из Twitter. Проверим это через Hue. Заходим в папку, которую мы указали для сбора логов (/user/cloudera/tweets).

12. Если файлы не появились начинаем с проверки кофигурации Flume. Через основное меню идем в  Flume, далее в Instance, в нашем случае запущен один агент на одном узле (может быть несколько и они могут быть запущены на разных узлах, более того на одном узле может быть запущено более одного агента). Переходим к Agent, далее Log Files -> Role Log File.

## Практическое задание 4 (Apache Kafka)

1. Создадим топик с одной партицией со степенью репликации 1 (т.к. у нас один узел в обучающем кластере). Запишем туда сообщение, после чего это сообщение прочтем. Далее прочтем все сообщения, которые есть в этом топике с самого начала и следующим шагом модифицируем конфигурацию Flume так, чтобы он начал писать еще и в Kafka. Т.е. мы будем собирвть архив данных в HDFS и при этом параллельно данные будут реплицироваться в Kafka для дальнейшего анализа. Посмотрим, что из твиттера пишется в реальном времени в Kafka.

2. Работать будем через консоль. Посмотрим, какие топики уже существуют. Это команда `kafka-topics` с параметром `--list`. Кроме того команда ждет от нас указания zookeeper (сервис для хранения конфигураций) и нам нужно знать к какому kafka-кластеру мы подключаемся. 2181 - стандартный порт для zookeeper.
   
   > `kafka-topics` # справка по команде
   > 
   > `kafka-topics --zookeeper quickstart.cloudera:2181/kafka --list` # просмотр топиков

3. Видим сообщения, но топиков нет.

4. Создадим топик tweets.
   
   > `kafka-topics --zookeeper quickstart.cloudera:2181/kafka --create --topics tweets --partition 1 --replication-factor 1 ` # создание топика

5. Ждем какое-то время и видим, что топик создан.

6. Снова проверяем список топиков, которые есть. Видим топик tweets.

7. Запишем что-нибудь в топик tweet с помощью команды `kafka-console-produser`. Этой утилите необходимо указать какой брокер отправляет сообщение, его порт будет 9020 (можно посмотреть в конфигурации), и указываем в какой топик писать (в наш tweets). Т.е. мы пишем в конкретный брокер, который находится на этом узле и пишем в топик tweets.
   
   > `kafka-console-produser --broker-list quickstart.cloudera:9092 --topic tweets ` # доступ к отправке сообщения в Kafka через консоль

8. Открывается приглашение к записи сообщения `>`, в этой строке пишем `hello world`, `hi big data`

9. Чтобы понять, что мы отправляем сообщения - открываем еще одно окно PuTTy, подключаемся к тому же топику и читаем сообщения.

10. Нажимаем правой копкой по PuTTy, выбираем duplicate session, логинимся в нем. Запускаем `kafka-console-consumer`, который позволяет читать из Kafka. Нужно указать kafka-кластер, каким брокером пишем в данном случа не важно и из какого топика читать.
    
    > `kafka-console-consumer --zookeeper quickstart.cloudera:2181/kafka --topic tweets` # чтение из топика kafka

11. Теперь при вводе сообщения в первом окне PuTTy они дублируются во втором окне.

12. Теперь нам бы хотелось прочитать и те сообщения, которые мы написали ранее. Для этого указываем опцию `-- from-beginning`
    
    > `kafka-console-consumer --zookeeper quickstart.cloudera:2181/kafka --topic tweets --from-beginning` # чтение всех сообщений из топика kafka

13. При этом, в этом режиме мы продолжаем слушать сообщения налету.

14. Внесем изменения в конфигурацию Flume:
    
    - ДОБАВЛЯЕМ ВТОРОЙ КАНАЛ, КОТОРЫЙ КОТОРЫЙ ОТПРАВЛЯЕТ ДАННЫЕ В KAFKA
    
    - ПОЯВЛЯЕТСЯ ВТОРОЙ СТОК Kafka
    
    - ДОБАВЛЯЕМ КАНАЛ KAFKA В TWITTER, ЧТОБЫ ТУДА ПОСТУПАЛИ ДАННЫЕ
    
    - ДОБАВЛЯЕМ КАНАЛ KAFKA
    
    - ДОБАВЛЯЕМ СТОК ДЛЯ KAFKA
    
    ```textile
    # vibo: откуда поступают данные (Twitter)
    # Имя_нашего_агента.источники_данных
    TwitterAgent.sources = Twitter
    
    # vibo: что мы будем использовать в качестве канала (канал в оперативной памяти)
    # Имя_нашего_агента.каналы_передачи_данных
    # ДОБАВЛЯЕМ ВТОРОЙ КАНАЛ, КОТОРЫЙ КОТОРЫЙ ОТПРАВЛЯЕТ ДАННЫЕ В KAFKA
    TwitterAgent.channels = MemChannel MemChannelKafka
    
    # vibo: куда будем сливать наши данные (в HDFS)
    # Имя_нашего_агента.стоки
    # ПОЯВЛЯЕТСЯ ВТОРОЙ СТОК Kafka
    TwitterAgent.sinks = HDFS Kafka
    
    ########## Source Twitter ##########
    
    # Имя_нашего_агента.источники_данных.Twitter.класс_отвечающий_за_источник
    TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSourse
    
    # Имя_нашего_агента.источники_данных.Twitter.название_канала
    # ДОБАВЛЯЕМ КАНАЛ KAFKA В TWITTER, ЧТОБЫ ТУДА ПОСТУПАЛИ ДАННЫЕ
    TwitterAgent.sources.Twitter.channels = MemChannel MemChannelKafka
    
    TwitterAgent.sources.Twitter.consumerKey = xxxxxxxxxxxxxxxxxxxx
    TwitterAgent.sources.Twitter.consumerSecret = xxxxxxxxxxxxxxxxxxxx
    TwitterAgent.sources.Twitter.accessToken = xxxxxxxxxxxxxxxxxxxx
    TwitterAgent.sources.Twitter.accessTokenSecret = xxxxxxxxxxxxxxxxxxxx
    
    # Имя_нашего_агента.источники_данных.Twitter.ключевые_слова_по_нашей_теме_через_запятую
    TwitterAgent.sources.Twitter.keywords = 
    
    ########## SINK HDFS ##########
    
    # Имя_нашего_агента.стоки.HDFS.каналы_передачи_данных
    TwitterAgent.sinks.HDFS.channel = MemChannel
    
    # Имя_нашего_агента.стоки.HDFS.тип_стока
    TwitterAgent.sinks.HDFS.type = hdfs
    
    # Имя_нашего_агента.стоки.HDFS.путь_к_папке_назначения
    TwitterAgent.sinks.HDFS.hdfs.path = hdfs:///user/cloudera/tweets
    
    # Имя_нашего_агента.стоки.HDFS.префикс_для_временных_файлов
    TwitterAgent.sinks.HDFS.hdfs.inUsePrefix =.
    
    # Имя_нашего_агента.стоки.HDFS.формат_файла
    TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
    
    # Настройка частоты сброса и размер файла.
    TwitterAgent.sinks.HDFS.hdfs.batchSize = 10
    TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
    TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600
    TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
    
    ########## CHANNEL ##########
    
    # Имя_нашего_агента.каналы.MemChannel.тип_канала
    TwitterAgent.channels.MemChannel.type = memory
    
    # Имя_нашего_агента.каналы.MemChannel.объем канала
    TwitterAgent.channels.MemChannel.capacity = 100
    
    # ДОБАВЛЯЕМ КАНАЛ KAFKA
    
    ########## ADDED KAFKA CHANNEL ##########
    TwitterAgent.channels.MemChannelKafka.type = memory
    TwitterAgent.channels.MemChannelKafka.capacity = 100
    
    # ДОБАВЛЯЕМ СТОК ДЛЯ KAFKA
    
    ########## SINK KAFKA ##########
    # vibo: стандартный sink для flume, может сразу писать в kafka
    TwitterAgent.sinks.Kafka.type = org.apache.flume.sink.kafka.KafkaSink
    # vibo: указываем наименование топика
    TwitterAgent.sinks.Kafka.topic = tweets
    # vibo: указываем брокера, который будем писать
    TwitterAgent.sinks.Kafka.brokerList = quickstart.cloudera:9092
    # vibo: указываем канал, через который это делать
    TwitterAgent.sinks.Kafka.chennel = MemChannelKafka
    # vibo: и указываем сколько сообщений
    TwitterAgent.sinks.Kafka.batchSize = 20
    ```

15. Обновляем конфигурацию в Flume (Configuration File). Сохраняем изменения. Рестартуем кофигурацию Flume. Проверяем, пишутся ли сообщения к Kafka (PuTTy, второй терминал).

## Практическое задание 5 (Hive и Impala)

1. Нужно создать таблицу над теми данными, которые мы получаем из твиттера, которые сейчас лежат в HDFS в виде json-файлов. Провести их обработку. В Hive будем считать, сколько сообщений лежит в каждом файле и сколько всего сообщений успело накопиться в нашем архиве. В Impala мы посчитаем количество строк, но посмотрим разницу между двумя этими системами, как они работают с данными, которые постоянно поступают.

2. Начинаем с Hive. Чтобы создать в нем таблицу переходим в Hue, выбираем Editors, во вкладке Documents выбираем Hive. Сразу попадаем в базу данных по умолчанию (называется default). Можем в нее зайти, здесь пока не создано ни одной таблицы.

3. Создать таблицу можно вручную, или используя sql-синтаксис. Воспользуемся sql, создадим таблицу SRC_TWEETS с одним столбцом full_tweet, формат String. Далее говорим, как у нас хранятся эти данные (TEXTFILE), если бы они лежали в каким-то формате мы бы написали STORED AS PARQUET, STORED AS SRC. Последнее, что нужно указать location, т.е. в какой папке лежат эти данные (можно с префиксом hdfs, можно без него, hive поймет, что файлы на распределенной файловой системе). В этом примере мы создали таблицу **непрерывно связанную с данными**. Если потребуется удалить таблицу, то и данные удалятся вместе ней.
   
   ```sql
   CREATE TABLE IF NOT EXISTS SRC_TWEETS (full_tweet String)
   STORED AS TEXTFILE
   LOCATION '/user/cloudera/tweets';
   ```

4. Добавляем **EXTERNAL (подобие view, которая смотрит на данные)** - отвязываем метаданные от самих данных и теперь при удалении таблицы (т.е. ее метаданных) удалится только ее описание, а не сами данные; они как лежали в HDFS, так и останутся, можн овсегда создать еще и еще разные таблицы.
   
   ```sql
   CREATE EXTERNAL TABLE IF NOT EXISTS SRC_TWEETS (full_tweet String)
   STORED AS TEXTFILE
   LOCATION '/user/cloudera/tweets';
   ```

5. Нажимаем на треугольник для создания таблицы. Чтобы ее увидеть слева нажимаем refresh.

6. Посмотрим, что в ней лежит с ограничением в 10 записей. Видим, что каждый твит уже лежит отдельно в своей ячейке.
   
   ```sql
   SELECT * from src_tweets LIMIT 10;
   ```

7. Посмотрим сколько твитов в каждом файле. Для этого у Hive есть специальная системная колонка (INPUT__FILE__NAME), которая позволяет взять имя файла из которого мы берем информацию. Считаем сколько строк и группируем по каждому файлу. Таким образом, узнаем сколько твитов лежит в каждом файле. После запуска выполяется MapReduce-программа (пиктограмма с самолетом обозначает, что программа еще выполняется). В правом верхнем углу можно вывести логи. Видим, что запустилась job, также есть ссылка, чтобы посмотреть подробности. Также здесь указан % выполнения map и reduce задачи.
   
   ```sql
   SELECT INPUT__FILE__NAME, count(full_tweet) FROM src_tweets
   GROUP BY INPUT__FILE__NAME;
   ```

8. Простой запрос с группировкой занимает значительное время. Посмотрим потом как это можно ускорить в Impala. А здесь движок MapReduce.

9. Посмотрим сколько всего строк собралось (сколько твитов). Получили порядка 56 тыс. твитов.
   
   ```sql
   SELECT count(full_tweet) FROM src_tweets;
   ```

10. Как мы видим запросы в Hive выполняются долго. Для аналитики это не очень подходит, для формирования отчетов вполне. При этом сейчас наши данные хранятся не оптимально, простым текстом, без всякого сжатия, это не поколоночное хранение. Мы могли бы положить данные в CSV, любой другой файл и выполнять над ним sql. Мы могли бы рассказать как разбить json на ячейки и писать запрос к конкретным колонкам.

11. Попробуем сделать все тоже самое через Impala. Переходим в другой Editor -> Impala. У нас единая база данных default для Hive и Impala. Но табличку на текущий момент мы не видим. Чтобы ее увидеть нам нужно синхронизировать метаданные Impala c Hive. Impala берет эти данные на определенный момент времени и не знает, что они меняются в Hive. Нажимаем на значек refresh и выбираем `Perform incremental metadata update` или `Invalidate all metadata and rebuild index`(все заново выкачивает). После этого дложна появиться табличка src_tweets в Impala.

12. Собираем статистику, чтобы запросы выполнялись быстрее. COMPUTE STATS позволяет просканировать таблицу и построить по ней индексы.
    
    ```sql
    COMPUTE STATS src_tweets;
    ```

13. Теперь делаем запрос count() в Impala. Запрос выполняется практически мгновенно. Теже 56200 твитов.
    
    ```sql
    SELECT count(full_tweet) FROM src_tweets;
    ```

14. Переходим обратно в Hive, делаем снова запрос - количество твитов за это время увеличилось. Возращаемся в Impala, делаем запрос без обновления таблицы - значение твитов не изменилось - 56200. Hive медленно, но выдает актуальную информацию. Impala - выдает статистику по срезу на определенный момент и показывает его.

## Практическое задание 6 (Apache Spark)

1. Выполним wordcount, используя Spark и Python. После чего те данные, которые у нас грузились в Kafka мы будем потоково в Spark получать, структурировать и сохранять в таблицу в формате паркет. Проверим, что данные появились и посчитаем количество друзей у каждого пользователя. Работать со Spark будем через консоль.

2. Открываем Putty, подключаемся. Выполняем команду `pyspark2`
   
   > `pyspark2` # запуск спарка для работы через интерактивную консоль

3. Будем работать с последней версией спарка 2.0. Мы запустили приложение через консоль, посмотрим, как это выглядит в Cloudera Manager на кластере. Переходим во вкладку Clusters -> YARN Applications. Видим, что теперь у нас есть PySparkShell (ему присоен номер), пока мы работаем в консоли, на кластере выполняется приложение.

4. Данные для подсчета количества слов будем брать также из дирректории /user/cloudera/example_tweets, файл example.json. Прочитаем этот файл в переменную spark (sc - spark context). Напомним, что Spark выполнит запрос, когда мы его попросим материализовать данные.
   
   > `text_file = sc.textFile("hdfs:///user/cloudera/example_tweets")`

5. Попросим спарк показать, что что-то произошло. Для этого возьмем первую строчку из файла text_file:
   
   > `text_file.take(1)` # в этот момент происходит выполнение запроса

6. Если посмотреть во вкладке Clusters -> YARN Applications -> PySparkShell -> application переходим во вкладку Hadoop, здесь нажимаем на ApplicationMaster, попадаем в интерфейс Spark, а не как раньше в MapReduce задание. В логах можно увидеть, что Python вызывает для работы Scala.

7. Во вкладке Executors можно посмотреть кто выполнял работу: есть один driver и executor, который и выполнял работу.

8. Выделим слова, разбиваем каждую строку по пробелам. Берем переменную `text_file`, вызываем функцию `flatMap` (считает количество слов в одной строке). Говорим, что на входе у нас будет строка, а на выходе - строка разбитая через пробел (split)
   
   > `words = text_file.flatMap(lambda line: line.split(" "))`
   > 
   > `words.take(1)`

9. На вкладке Spark можем посмотреть DAG Visualization.

10. Создаем переменную word_count, точнее пару, которая состоит из слова и переменной 1 (пара ключ - значение)
    
    > `words_count = words.map(lambda word: (word, 1))`

11. Далее считаем с помощью команды `reduceByKey`, которая возьмет на вход все наши одинаковые слова и сложит 1. Сохраним результат.
    
    > `counts = words_count.reduceByKey(lambda a,b: a+b)`
    > 
    > `counts.saveAsTextFile("hdfs:///user/cloudera/spark_wc_result")`

12. Чтобы посмотреть результат идем в Hue, папка `spark_wc_result`. Получили 8 файлов, т.к. у нас было 8 executors. Видим, что пример на Spark занимает намного меньше места, чем на java. Мы не брали готовую библиотеку, написали находу.

13. Во второй части подключаемся к Kafka и пробуем выкачать из нее твиты. Снова запускаем pyspark2. Нам потребуется импортировать ряд библиотек, чтобы подключаться к Kafka, они идут в комплекте spark и т.к. твиты идут постоянно, то мы будем использовать SparkStreaming, который позволяет работать с потоком данных. Подключим модуль для работы с потоками данных, модуль по работе с Kafka. Импортируем библиотеку `json`, которая может конвертировать текст json в объект и обратно.
    
    > `from pyspark.streaming import StreamingContext`
    > 
    > `from pyspark.streaming.kafka import KafkaUtils`
    > 
    > `import json`

14. Подключаемся к SparkStreaming, и обновляем данные каждые 60 секунд. Т.е. 60 секунд накапливаем данные, потом начинаем их обрабатывать.
    
    > `streamingContext = StreamingContext(spark.sparkContext, 60)`

15. Подключаемся к Kafka, используя созданный выше streamingContext, указываем топик, который будем смотреть (tweets), также говорим куда подключиться - указываем список брокеров.
    
    > `directKafkaStream = KafkaUtils.createDirectStream(streamingContext, ["tweets"], {"metadata.broker.list": "quickstart.cloudera:9092"})`

16. Для накопленной информации за 60 секунд выполним преобразование - выдели твиты; directKafkaStream - это будет набор данных RDD, которые к нам вернутся. Для накопившихся данных за 60 секунд мы выполним функцию map, которая на вход получит одно сообщение из Kafka и возьмем из него только первый объект. Через библиотеку json делаем объект, его и возвращаем.
    
    > `tweets = directKafkaStream.map(lambda v: json.loads(v[1]))`
    > 
    > `tweets.pprint()` # вывод первых 10 записей
    > 
    > `streamingContext.start()` # запуск Streaming

17. Останавливаем выполнение.

18. Теперь мы хотим сохранить информацию в виде таблицы для обработки Impala. Сначала сохраним таблицу в Hive, потом перейдем в Impala. У нас уже есть заготовка, которая подключается к Kafka и начинает из нее выкачивать данные. Данные возвращаются в качестве объектов - твитов, нам необходимо из них сделать таблицу. Возьмем готовую схему - /user/cloudera/schema_example.json. В данном файле наиболее полно описана схема из документации твиттера, что он возвращает и тип данных.

19. Запускаем заново spark2, импортируем библиотеки StreamingContext, KafkaUtils, json. Возьмем пример tweet_example. Прочитаем его, получмим из него табличку. Используя spark session прочтем этот файл в формате json и сразу сделаем из него табличное представление.
    
    > `tweet_example = spark.read.load("hdfs:///user/cloudera/schema_example.json", "json")`

20. Скажем как будет называться наша таблица и укажем путь
    
    > `db_table = "default.tweets"`
    > 
    > `table_path = "hdfs:///user/cloudera/streamed-tweets-parquet"`

21. Далее нам нужно понять существует ли наша таблица. Если существует, то нужно дописывать данные, если не существует ее нужно создать. spark.catalog.listTables() - позволяет посмотреть какие таблицы у нас есть, проходя по нему циклом мы сравниваем названия таблицы с нашим названием и если оно есть, то в listTables что-то появится.
    
    > `table_exists = [table for table in spark.catalog.listTables() if table.database+'.'+table.name == db_table]` 

22. Напечатаем table_exists. Видим [] - папка пустая
    
    > `table_exists`

23. Теперь запишем, если наша таблица не существует - создаем пустой датафрейм и берем схему данных из примера твитта. Создаем таким образом таблицу.
    
    > `if not table_exists:`
    > 
    > `df = spark.createDataFrame(sc.emptyRDD(), tweet_example.schema)`
    > 
    > `df.write.saveAsTable(db_table, format='parquet', mode='overwrite', path='table_path')`

24. Проверяем через Hue, создалась ли таблица. Переходим в Hive, обновляем. Проверяем наличие таблицы tweets. При нажатии на нее видим ее структуру, из примера твита. Посмотрим что-нибудь (должно быть пусто):
    
    ```sql
    SELECT * FROM tweets;
    ```

25. Создадим функцию, которая будет записывать данные из Kafka в эту таблицу (пишем в PuTTy). Принимает на вход RDD, если RDD пустой, то ничего не делаем, а если не пустой - то мы из него создаем датафрейм, используя схему примера и дописываем в нашу табличку. В конце возращаем RDD.
    
    ```python
    def saveToTable(rdd):
        try:
            # Проверяем, что RDD не пустой
            if not rdd.isEmpty():
                # Создаем DataFrame. В качестве данных - RDD, в качестве схемы - схем примера
                df = spark.creatDataFrame(rdd, tweet_example.schema)
                # Дописываем в таблицу defualt.tweets
                df.write.mode("append").insertInto(db_table)
        except Exception as e:
            print("Ooops!", e)
        return rdd
    ```

26. Возвращаемся в Kafka и вызываем для каждого набора данных нашу функцию.
    
    > `streamingContext = StreamingContext(spark.sparkContext, 60)`
    > 
    > `directKafkaStream = KafkaUtils.createDirectStream(stremingContext, ["tweets"], {"metadata.broker.list": "quickstart.cloudera:9092"})`
    > 
    > `tweets = directKafkaStream.map(lambda v: json.loads(v[1]))`
    > 
    > `tweets.foreachRDD(lambda x: saveToTable(x))`
    > 
    > `streamingContext.start()` # запуск Streaming
    > 
    > `streamingContext.awaitTermination()` # ждем прерывания

27. Идем в Cloudera Manager, YARN -> Applications -> Spark -> Streaming. Первая итеррация завершена, можем посмотреть наполнение таблицы через Hue в Hive:
    
    ```sql
    SELECT * FROM tweets;
    ```

28. Видим, что таблица уже не пустая. Данные вставляются благодаря приложению на спарке, которое мы написали. Промышленные приложения запускаются не так. Мы посмотрели скорее на способ отладить наше приложение, посмотреть как оно работает. Для запуска мы все, что было написано оборачиваем в питоновский файл и запускаем через spark submit, направляем на кластер, там же создается драйвер и выполняется этот код.

29. Перейдем из Hive в Impala. Найдем среднее количество друзей. Обновляем таблички (`Perform incremental metadata update`). Ждем пока в Impala появится таблчка Tweets. Посмотрим, что импортировалось (по хорошему перед этим нужно было посчитать статистику `COMPUTE STATS tweets` ):
    
    ```sql
    SELECT * FROM tweets;
    ```

30. User представляет собой структуру, состоящую из нескольких атрибутов, у него есть поле friends_count. Выведем имена пользователей и их количество друзей.
    
    ```sql
    SELECT user.name, user.friends_count FROM tweets;
    ```

31. Найдем среднее количество друзей, которые интересуются нашей темой (настройка - ключевые слова), в среднем 2000 друзей.
    
    ```sql
    SELECT avg(user.friends_count) as friends_avg FROM tweets;
    ```

## Практическое задание 7 (Solr)

1. Знакомимся с инструментом Solr (поисковой движок). Мы создадим специальную конфигурацию для хранения твитов, после чего зададим ее в Solr и попытаемся туда записать какие-то тестовые данные. Посмотрим, что они записались, поищем по ним. Перейдем в графический интерфейс Solr. Смотрим, какие у нас есть Instance, переходим в `Solr Server Web UI`. Видмим, что у нас нет ни одного ядра, их нужно создать. Делать это будем через консоль. Команда solrctl позволяет работать с solr, создавать так называемые ядра.
   
   > `solrctl` # вывод подсказки

2. В самом начале будем работать с `instancedir`, т.е. создадим специальную папку (tweets_conf_test), в которую сгенерируем нашу конфиграцию, базовую конфигурацию, которую потом поменяем и уже на основе нее создадим наше поисковое ядро, которое будет работать в соответствии с данной конфигурацией работать. `-schemaless` - схема будет подстраиваться под данные (будет плавающая). Далее создаем конфигурацию tweets, используя папку tweets_conf_test. Это значит, что мы загружаем конфигурацию в zookeeper для того, чтобы она была доступна нескольким серверам.
   
   > `solrctl instancedir --generate tweets_conf_test -schemaless`
   > 
   > `solrctl instancedir --create tweets tweets_conf_test/`

3. Посмотрим на конфигурацию, идем в папку tweets_conf_test/conf/ нас интересует schema.xml и solrconfig.xml. Еще есть файлы со стоп-словами, синонимами, все то, что может нам улучшить обработку наших слов.
   
   > `nano schema.xml` # просмотр содержимого

4. schema.xml - это файл с описанием, который говорит нам как разбирать слова, как их парсить.

5. Создадим коллекцию для твитов (-zk это zookeeper). Говорим адрес текущей машины, конфигурацию которую будем использовать.
   
   > `solrctl --zk quickstart:2181/solr collection --create tweets -c tweets`

6. Возвращаемся в графический интерфейс Solr, видим, что у нас создалось ядро  tweets, состоящее из одного shard, который находится на машине quickstart.cloudera.

7. Запишем пример данных в Solr, будем использовать питон. На виртуальной машине установлена анаконда, можно вызвать python 3.6. Импортируем библиотеку pysolr, с помощью которой будем загружать туда данные.
   
   > `/opt/anaconda/bin/python`
   > 
   > `import pysolr`

8. Подключаемся к zookeeper, указываем адрес. Подключаем нашу коллекцию - tweets_collection, используем SolrCloud и говорим, что наша коллекция называется tweets.
   
   > `zookeeper = pysolr.ZooKeeper("quickstart.cloudera:2181/solr")`
   > 
   > `tweets_collection = pysolr.SolrCloud(zookeeper, "tweets")`

9. Добавляем набор записей, которые имеют id и имя
   
   > `tweets_collection.add([("id":1,"name":"test1"), ("id":2,"name":"test2"),  ("id":3,"name":"test3")])`

10. Переходим в графический интерфейс, видим, что у нас появилось три новых документа. Можно зайти в Query, выполнить запрос и увидеть наши данные. Такой интерфейс не очень удобный, переходим в Hue. Переходим в Dashbord, где можем вывести таблицу. Построить график. Можно фильтровать данные просто нажимая на сектор диаграммы.

## Практическое задание 8 (сборка приложения)

1. Соберем все, добавим обогощение данных. У нас уже собираются твиты в Kafka, мы уже научились их забирать Spark во время стриминга, и создавать таблички в Impala. Научились немного работать в Solar, класть тестовые документы. Посмотрели, как в Hue можно создать простое приложение, чтобы эти данные показать. Сейчас соберем все это вместе. Данные, которые поступают из твиттера через Flume в Kafka, брать их Sparkom в SparkStreaming далее обогощать их используя специальную питоновскую библитеку, которая позволяет выделять лица на фотографиях. Будем определять сколько людей и есть ли вообще люди на фотографиях. И если люди на фотографиях accaunt твиттера есть, то мы эти данные будем отправлять в Solar из питона и далее сделаем небольшое приложение в Hue, которое позволит посмотреть сколько людей на фотографиях, и что они писали.

2. Почистим тестовые записи, которые мы сделали на предыдущем этапе, очистим tweets в Solr.
   
   > `solrctl --zk quickstart:2181/solr collection --deletedocs tweets`

3. Обновляем страницу Solr, видим, Num Docs: 0

4. На всякий случай зайдем в Hue, обновим страницу и убедимся, что данных нет.

5. Напишем питоновскую программу. Часть индексирования в Solar мы уже напили выше. В части забора данных из Kafka тоже. Нам осталось написать фугкцию, которая будет брать и данных твиттера ссылку на фотографию, эту фотку выкачивать, обогощать ее питоновской библиотекой - есть ли там лица. и создавать некий документ, который мы будем отправлять в Solar.

6. Программа, которая сохраняла наши данные в таблицу
   
   ```python
   from pyspark.streaming import StreamingContext
   from pyspark.streaming.kafka import KafkaUtils
   from pyspark.sql import SparkSession
   import json
   
   spark = SparkSession.builder.appName("Python Spark Streaming").getOrCreate()
   # загрузка схемы данных
   tweet_example = spark.read.load("hdfs:///user/cloudera/schema_example.json", "json")
   # название таблицы, куда мы будем сохранять
   db_table = "default.tweets"
   # путь к таблице
   table_path = "hdfs:///user/cloudera/streamed-tweets-parquet"
   # проверяем есть ли таблица: для каждой таблицы в spark.catalog.listTables() проверяем совпадает ли ее имя с нашей, если да, то возвращаем список совпавших
   table_exists = [table for table in spark.catalog.listTables() if table.database+'.'+table.name == db_table]
   # если список пустой - таблица не существует (создаем ее), если нет -существует
   if not table_exists:
       # создаем пустой DataFrame
       df = spark.createDataFrame(sc.emptyRDD(), tweet_example.schema)
       # создаем таблицу
       df.write.saveAsTable(db_table, format='parquet', mode='overwrite', path='table_path')
   # создаем контекст Spark Streaming c обновлением каждые 60 секунд
   streamingContext = StreamingContext(spark.sparkContext, 60)
   # подключаемся к Kafka, топик tweets, брокер quickstart.cloudera:9092
   directKafkaStream = KafkaUtils.createDirectStream(streamingContext, ["tweets"], {"metadata.broker.list": "quickstart.cloudera:9092"})
   # для накопившихся за 60 секунд данных (k, v) взять занчения (v) и перевести их в объект из json
   tweets = directKafkaStream.map(lambda v: json.loads(v[1]))
   
   def saveToTable(rdd):
       try:
           # Проверяем, что RDD не пустой
           if not rdd.isEmpty():
               # Создаем DataFrame. В качестве данных - RDD, в качестве схемы - схем примера
               df = spark.creatDataFrame(rdd, tweet_example.schema)
               # Дописываем в таблицу defualt.tweets
               df.write.mode("append").insertInto(db_table)
       except Exception as e:
           print("Ooops!", e)
       return rdd
   
   tweets.foreachRDD(lambda x: saveToTable(x))
   # запустить Streaming
   streamingContext.start()
   # ждать прерывания
   streamingContext.awaitTermination()
   ```

7. Модифицируем ее. Перенесем приложение в файл и запустим на кластере. **Начнем с выкачки фотографий.**  

8. Добавляем функцию, которая будет индексировать в Solr (indexToSolr() - эту функцию нужно написать), вставляем рядом с tweets.foreachRDD(lambda x: saveToTable(x)). 
   
   ```python
   tweets.foreachRDD(lambda x: indexToSolr(x))
   ```

9. На вход она должна принимать RDD. Для каждого RDD мы будем сохранять результат с Solr. Функция indexToSolr будет проверять если RDD не пустой, то как и в предыдущем примере подключаемся к zookeeper, коллекции tweets и для каждого твита нам нужно сходить по определенному адресу, взять оттуда фотографию и распознать на ней лица и если там есть лица отправить твит в Solr.
   
   ```python
   def indexToSolr(rdd):
       try:
           # vibo: если rdd не пустой
           if not rdd.isEmpty():
               # vibo: вызываем функцию, которая для каждой записи в rdd (для каждого твита) вызывает get_solr_doc()
               # vibo: готовим документ для Solr
               docs = rdd.map(lambda tweet: get_solr_doc(tweet))
               # vibo: фильтруем, чтобы оставить все не пустые документы
               for_solr = docs.filter(lambda doc: doc != None)
               # vibo: если такие документы остались
               if not for_solr.isEmpty():
                   # vibo: вызываем функцию mapPartitions, которая для каждой партиции выполняет вставку в Solr
                   # vibo: для каждой партиции, чтобы создавать одно подключение с каждого узла, чтобы не создавать новое подключение для каждой записи
                   # vibo: count() чтобы заставить Spark работать
                   for_solr.mapPartitions(lambda partition: insert_into_solr(partition)).count()
   
       except Exception as e:
           print("Ooops solr!", e)
       return rdd
   ```

10. Пишем функцию, которая нам должна подготовить документ
    
    ```python
    # vibo: подготовка документа, на вход функция получает твит
    def get_solr_doc(tweet):
       # формируем url фотографии профиля
       user_url = 'https://twitter.com/{screen_name}/profile_image?size=original'.format(
           screen_name=tweet['user']['screen_name'])
       # выкачиваем фотографию
       img_content = requests.get(user_url).content
       # определяем количество лиц на фотографии
       faces = getFacesCount(img_content)
       if faces > 0:
           # готовим документы
           return {'id': tweet['id'], \
                           'user_screen_name': tweet['user']['screen_name'], \
                           'user_name': tweet['user']['name'], \
                           'user_url': user_url, \
                           'user_base64_img': get_as_base64(img_content), \
                           'faces_count': faces, \
                           'text': tweet['text'], \
                           'user_lang': tweet['user']['lang'], \
                           'user_friends_count': tweet['user']['friends_count']}
       else:
           return None
    ```

11. Можем посмотреть через графический интерфейс Impala, какие фотографии профилей у нас есть. Сначала получаем user.screen_name, потом ставляем user.screen_name вместо {screen_name}. Открываем полученную ссылку в браузере.
    
    ```sql
    SELECT user.screen_name from tweets;
    ```
    
    ```sql
    https://twitter.com/{screen_name}/profile_image?size=original
    ```

12. Функция, которая считает лица на фотографии использует специальную библиотеку face_recognition. Передаем ей нашу фотографию и просим получить все расположения лиц, считаем их количество.
    
    ```python
    def getFacesCount(img_content):
        # определяем сколько лиц на фотографии
        image = face_recognition.load_image_file(BytesIO(img_content))
        return len(face_recognition.face_locations(image))
    ```

13. Соответственно, если лицо есть - готовим документ, если нет, возвращаем None. Документ в нашем случае не весь твит, а только определенные поля, которые нам интересны:
    
    - user_screen_name ()
    
    - user_name (имя пользователя)
    
    - user_url (ссылка с которой взяли фотографию)
    
    - user_base64_img (сама фотография)
    
    - faces_count (сколько лиц на фотографии)
    
    - text (собственно текст твита)
    
    - user_lang (язык текста)
    
    - user_friends_count (количество друзей)

14. Функция кодирования фотографии для отображения в Hue
    
    ```python
    def get_as_base64(img_content):
        # кодируем данные при помощи только 64 символов ASCII
        return base64.b64encode(img_content)
    ```

15. Напишем функцию insert_into_solr:
    
    ```python
    def insert_into_solr(for_solr_docs):
        # подключаемся к Zookeeper
        zookeeper = pysolr.ZooKeeper("quickstart.cloudera:2181/solr")
        # получаем коллекцию
        solr = pysolr.SolrCloud(zookeeper, "tweets")
        # индексируем документы
        solr.add(for_solr_docs)
        return for_solr_docs
    ```

16. Добавляем библиотеки, которые будем использовать

```python
import pysolr
import face_recognition
from io import BytesIO
import base64
import requests
```

17. Приложение готово.

18. Переходим в терминал PuTTy, создаем файл spark_demo.py
    
    > `nano spark_demo.py`

19. Вставляем код приложения:

```python
from io import BytesIO
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils
from pyspark.sql import SparkSession
import json
import pysolr
import face_recognition
from io import BytesIO
import base64
import requests

spark = SparkSession.builder.appName("Python Spark Streaming").getOrCreate()
# загрузка схемы данных
tweet_example = spark.read.load("hdfs:///user/cloudera/schema_example.json", "json")
# название таблицы, куда мы будем сохранять
db_table = "default.tweets"
# путь к таблице
table_path = "hdfs:///user/cloudera/streamed-tweets-parquet"
# проверяем есть ли таблица: для каждой таблицы в spark.catalog.listTables() проверяем совпадает ли ее имя с нашей, если да, то возвращаем список совпавших
table_exists = [table for table in spark.catalog.listTables() if table.database+'.'+table.name == db_table]
# если список пустой - таблица не существует (создаем ее), если нет -существует
if not table_exists:
   # создаем пустой DataFrame
   df = spark.createDataFrame(sc.emptyRDD(), tweet_example.schema)
   # создаем таблицу
   df.write.saveAsTable(db_table, format='parquet', mode='overwrite', path='table_path')
# создаем контекст Spark Streaming c обновлением каждые 60 секунд
streamingContext = StreamingContext(spark.sparkContext, 60)
# подключаемся к Kafka, топик tweets, брокер quickstart.cloudera:9092
directKafkaStream = KafkaUtils.createDirectStream(streamingContext, ["tweets"], {"metadata.broker.list": "quickstart.cloudera:9092"})
# для накопившихся за 60 секунд данных (k, v) взять занчения (v) и перевести их в объект из json
tweets = directKafkaStream.map(lambda v: json.loads(v[1]))

def saveToTable(rdd):
   try:
       # Проверяем, что RDD не пустой
       if not rdd.isEmpty():
           # Создаем DataFrame. В качестве данных - RDD, в качестве схемы - схем примера
           df = spark.creatDataFrame(rdd, tweet_example.schema)
           # Дописываем в таблицу defualt.tweets
           df.write.mode("append").insertInto(db_table)
   except Exception as e:
       print("Ooops!", e)
   return rdd

def get_as_base64(img_content):
   # кодируем данные при помощи только 64 символов ASCII
   return base64.b64encode(img_content)

def getFacesCount(img_content):
   # определяем сколько лиц на фотографии
   image = face_recognition.load_image_file(BytesIO(img_content))
   return len(face_recognition.face_locations(image))

# vibo: подготовка документа, на вход функция получает твит
def get_solr_doc(tweet):
   # формируем url фотографии профиля
   user_url = 'https://twitter.com/{screen_name}/profile_image?size=original'.format(
       screen_name=tweet['user']['screen_name'])
   # выкачиваем фотографию
   img_content = requests.get(user_url).content
   # определяем количество лиц на фотографии (функция getFacesCount)
   faces = getFacesCount(img_content)
   if faces > 0:
       # готовим документы
       return {'id': tweet['id'], \
                       'user_screen_name': tweet['user']['screen_name'], \
                       'user_name': tweet['user']['name'], \
                       'user_url': user_url, \
                       'user_base64_img': get_as_base64(img_content), \
                       'faces_count': faces, \
                       'text': tweet['text'], \
                       'user_lang': tweet['user']['lang'], \
                       'user_friends_count': tweet['user']['friends_count']}
   else:
       return None

def insert_into_solr(for_solr_docs):
   # подключаемся к Zookeeper
   zookeeper = pysolr.ZooKeeper("quickstart.cloudera:2181/solr")
   # получаем коллекцию
   solr = pysolr.SolrCloud(zookeeper, "tweets")
   # индексируем документы
   solr.add(for_solr_docs)
   return for_solr_docs

def indexToSolr(rdd):
   try:
       # vibo: если rdd не пустой
       if not rdd.isEmpty():
           # vibo: вызываем функцию, которая для каждой записи в rdd (для каждого твита) вызывает get_solr_doc()
           # vibo: готовим документ для Solr
           docs = rdd.map(lambda tweet: get_solr_doc(tweet))
           # vibo: фильтруем, чтобы оставить все не пустые документы
           for_solr = docs.filter(lambda doc: doc != None)
           # vibo: если такие документы остались
           if not for_solr.isEmpty():
               # vibo: вызываем функцию mapPartitions, которая для каждой партиции выполняет вставку в Solr
               # vibo: для каждой партиции, чтобы создавать одно подключение с каждого узла, чтобы не создавать новое подключение для каждой записи
               # vibo: count() чтобы заставить Spark работать
               for_solr.mapPartitions(lambda partition: insert_into_solr(partition)).count()
   except Exception as e:
       print("Ooops solr!", e)
   return rdd

tweets.foreachRDD(lambda x: saveToTable(x))
# vibo: добавляем функцию, которая будет индексировать в Solr
tweets.foreachRDD(lambda x: indexToSolr(x))
# запустить Streaming
streamingContext.start()
# ждать прерывания
streamingContext.awaitTermination()
```

20. Сохраняем, выходим
    
    > `Ctrl`+`O`, `Ctrl`+`X`

21. Запускаем приложение
    
    > `spark2-submit spark_demo.py`

22. Запускается приложение. Обновляем страницу Spark. Ждем пока наберется некоторое количество твитов. Т.к. наш кластер слабый может произойти так, что время на сбор твитов будет соизмерим со временем обработки. Это плохо и нужно будет добавлять ресурсов или распараллеливать.

23. Переходим в Hue, Dashboard. Переходим к настройке нашего приложения, чтобы было понятно, что мы собрали. Удаляем все объекты по умолчанию. Выбираем `One column layout`. Добавляем один столбец `HTML`. Правим HTML код в соответствующем разделе.
    
    ```html
    <table>
        <tbody><tr>
            <td width="400">
                <div class="row-fluid">
                    Name: <b>{{user_name}}</b> <br>
                    Screen_name: <b>{{user_screen_name}}</b> <br>
                    Friends: <b>{{user_friends_count}}</b> <br>
                    Language: <b>{{user_lang}}</b> <br>
                    Text: <b>{{text}}</b> <br>
    
                </div></td>
            <td aling="center" width="200">
                Detected <b>{{faces_count}} </b>face(s) on the picture:
                <br>
    
                <img src="data:image/png;base64, {{user_base64_img}}" width="100" height="100"></td>
        </tr>
    </tbody></table> 
    ```

24. В разделе CSS & JS правим стиль:
    
    ```css
    <style type="text/css">
    em {
        font-weight: bold;
    background-color: yellow;
    }
    
    table
    {
        table-layout: fixed;
        width: 100px;
    }
    
    td div { width: 50px; overflow: hidden; }
    </style>
    
    <script>
    </script> 
    ```

25. Добавим фильтр, который позволяет фильтровать данные по языку твита. Добавляем фильтр по количеству лиц на фото.

26. Т.к. мы еще пишем данные в таблицу Impala перейдем в нее посмотреть наполнение. Пишем запрос подсчета количества твитов (получили порядка 19 тыс.):
    
    ```sql
    SELECT count(*) from tweets;
    
    REFRESH tweets;
    ```

27. В Hue - Dashboard можно искать по тексту, в строке поиска пишем `text:СЛОВО`
