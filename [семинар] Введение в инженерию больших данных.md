НИТУ "МИСиС"

# Введение в инженерию больших данных

https://openedu.ru/

## Практическое задание 0 (Cloudera Manager, PuTTy)

1. Устанавливаем VirtualBox + putty (для доступа на удаленную машину).

2. Скачиваем образ виртуальной машины.

3. Импортируем в VirtualBox образ.
   
   <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-09-28-49-image.png" title="" alt="" data-align="center">

4. Запускаем cloudera-quickstart-vm-5.13.0.0-virtualbox
   
   <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-29-20-55-51-image.png" title="" alt="" data-align="center">

5. После запуска автоматически открывается cloudera.

6. Видим какие созданы узлы и их ip-адреса.

7. Для выхода из VirtualBox по умолчанию `Right Ctrl`

8. Переходим по ссылке Cloudera Manager, вводим логин и пароль (cloudera/cloudera)
   
   <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-29-20-59-31-image.png" title="" alt="" data-align="center">

9. Видим наш кластер
   
   <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-29-21-03-08-image.png" title="" alt="" data-align="center">

10. Переходим на вкладку Hosts - All Hosts
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-09-57-01-image.png" title="" alt="" data-align="center">
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-09-58-23-image.png" title="" alt="" data-align="center">

11. Нужно запустить сервисы и проверить, что они работают. Можно запускать сервисы по отдельности, можно все сразу. В разделе Cloudera QuickStart нажимаем `Start`, чтобы запустить все сразу.
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-00-57-image.png" title="" alt="" data-align="center">
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-05-05-image.png" title="" alt="" data-align="center">

12. Также в разделе Cloudera Managment Servise запустим сервис для управления кластером.
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-11-23-image.png" title="" alt="" data-align="center">
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-12-16-image.png" title="" alt="" data-align="center">

13. Переходим в браузере на `http://localhost:7180`. Снова вводим логин и пароль (cloudera/cloudera)
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-04-10-20-29-image.png" title="" alt="" data-align="center">

14. **Отключаем AdblockerUltimate и прочее не странице веб-браузера**. Теперь можно работать в браузерной версии ClouderaManager.
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-09-45-01-image.png" title="" alt="" data-align="center">

15. Чтобы подключиться к виртуальной машине через PuTTy нужно пробросить свободный порт. Заходим в настройки виртуальной машины в virtualbox Network -> Advanced -> Port Forwarding. Добавляем:
    
    - Name (SSH)
    
    - Protocol (TCP)
    
    - Host Port (например, 2222)
    
    - Guest IP (10.0.2.15, получаем через ifconfig в терминале виртуальной машины)
    
    - Guest Port (22)
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-11-15-52-image.png" title="" alt="" data-align="center">
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-11-21-21-image.png" title="" alt="" data-align="center">

16. Теперь можно подключаться через PuTTY. Запускаем `localhost` на порту `2222`, вводим логин/пароль (cloudera/cloudera).
    
    <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-10-05-11-32-24-image.png" title="" alt="" data-align="center">

17. Проверяем версию hadoop `hadoop version`.

18. В файле `hosts` добавляем строку `127.0.0.1 quickstart.cloudera` (локальный ip-адрес и имя).

19. Переходим в сервис Hue.

20. Переходим в графический интерфейс Hue (в адресе должен быть уже `quickstart.cloudera`). Вводим логин и пароль.

## Практическое задание 1 (HDFS, Hue)

1. В PuTTy набираем `hdfs`. Видим, что мы можем сделать с файловой системой.
   
   > `hdfs`  # список команд

2. Команда `hdfs dfs` - выводит список команд файловой системы HDFS.
   
   > `hdfs dfs` # список команд файловой системы

3. `hdfs dfs -help mkdir` - вывод справки по команде создания папки.
   
   > `hdfs dfs -help имя_команды` # вывод справки по команде

4. Создаем папку `hdfs dfs mkdir /user/cloudera/example_tweets`
   
   > `hdfs dfs mkdir имя_папки` # создание папки

5. Создаем вторую папку `hdfs dfs mkdir /user/cloudera/tweets`.

6. Меняем привилегии для второй папки. Выведем подсказку по команде `chmod` - `hdfs dfs -help chmod`. Разрешим всем писать в эту папку `hdfs dfs -chmod 777 /user/cloudera/tweets` (777 - разрешение).
   
   > `hdfs dfs -chmod 777 имя_папки` # изменение прав доступа к папке

7. Для просмотра используем команду `hdfs dfs -ls /user/cloudera`
   
   > `hdfs dfs -ls путь/имя_папки` # просмотр файлов дирректории

8. Аналогично можно посмотреть через браузер (сервис Hue).

9. Чтобы скопировать файл с локальной виртуальной машины используем команду  `hdfs dfs -put example.json /user/cloudera/example_tweets` (предварительно посмотреть размер файла можно командой `ls -lh`)
   
   > `ls -lh` # просмотр дирректории с правамми доступа, размером
   > 
   > `hdfs dfs -put имя_файла путь/имя_папки` # копирование файла

10. Аналогично можно проверить, что файл скопировался через Hue. На локальном компьютере просмотреть данный файл весьма проблематично (1 Gb), смотрим через Hue.

11. Создаем через Hue папку `test`. Добавляем файл, удаляем папку через web-интерфейс Hue.

## Практическое задание 2 (Wordcount)

1. Посчитаем количество слов в файле `example.json`.

2. Иcпользуем MapReduce, готовую реализацию подсчета слов из библиотеки hadoop examples. 

3. В корневой дирректории home/cloudera лежит файл hadoop-examples.jar, запустим его `hadoop jar hadoop-examples.jar`. Открывается перечень приложений.
   
   > `hadoop jar hadoop-examples.jar` # предустановленные приложения

4. Запускаем `hadoop jar hadoop-examples.jar wordcount /user/cloudera/example_tweets /user/cloudera/wordcount_result` (где лежит исходный файл и куда будем складывать результат). ВАЖНО! Папка назначения не должна существовать.
   
   > hadoop jar имя_архива_.jar имя_приложения путь/исходный_файл путь/итоговый_файл # запуск приложения

5. Файл делится на 8 splits, видим номер задачи (job). Также дается ссылка по которой можно контролировать выполнение job. Копируем ссылку в браузер.

6. Результат смотрим в папке `/user/cloudera/wordcount_result`. Здесь лежат два файла `_SECCESS` (метка, что процесс завершился успешно) и `part-r-00000` (результат 481,1 Mb). Буква r - говорит о том, что это результат reduce задачи (m - map).

7. Wordcount разбивает все по пробелам.

## Практическое задание 3 (Apache Flume)

1. Идем в Twitter, регистрируемся как разработчик, получаем ключи для создания приложения, которое будет получать потоковые данные из Twitter. Настроим Flume, чтобы он забирал информацию и раскладываел ее по файлам в HDFS. Приложение настроим на некую тематику по ключевым словам.

2. Идем на `apps.twitter.com` (apply for developer account), создаем приложение `bigdata_demo_application`. На вкладке `Keys and Access Tokens` для конфигурирования Flume получаем:
   
   - Consumer Key (API Key)
   
   - Consumer Secret (API Secret)
   
   - Access Token (нужно сгенерировать)
   
   - Access Token Secret (нужно сгенерировать)

3. Переходим к настройке Flume. В Clouder Manager переходим в сервис Flume, вкладка Configuration, раздел Configuration File и Agent Name.

4. Заполняем полк Agent Name - TwitterAgent.

5. Заполняем поле Configuration File (хотим вязть данные из Twitter и через оперативную память передать их в HDFS):
   
   ```textile
   # vibo: откуда поступают данные (Twitter)
   # Имя_нашего_агента.источники_данных
   TwitterAgent.sources = Twitter
   
   # vibo: что мы будем использовать в качестве канала (канал в оперативной памяти)
   # Имя_нашего_агента.каналы_передачи_данных
   TwitterAgent.channels = MemChannel
   
   # vibo: куда будем сливать наши данные (в HDFS)
   # Имя_нашего_агента.стоки
   TwitterAgent.sinks = HDFS
   ```

6. Продолжаем заполнять поле Configuration File - переходим к настройке источника данных. Настройка происходит следующим образом. Мы говорим название агента (TwitterAgent), далее говорим через точку источники (sources), название источника, который мя сейчас хотим настроить (Twitter), далее указываем:
   
   - .type = com.cloudera.flume.source.TwitterSourse (класс библиотеки, которая будет отвечать за сбор информации из Twitter; используем готовую библиотеку от Cloudera, импортированную на кластер). Если нужны другие источники - можно посмотреть на сайте Flume.
   
   - .channels = MemChannel (источник должен передавать данные в наш канал)
   
   - .consumerKey, .consumerSecret, .accessToken, .accessTokenSecret (получаем при регистрации приложения в режиме разработчика Twitter)
   
   - .keywords = russia, moscow, россия, москва (ключевые слова, которые мы хотим искать, Twitter будет пересылать сообщения, содержащие указанные слова)
   
   ```textile
   # Source Twitter
   
   # Имя_нашего_агента.источники_данных.Twitter.класс_отвечающий_за_источник
   TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSourse
   
   # Имя_нашего_агента.источники_данных.Twitter.название_канала
   TwitterAgent.sources.Twitter.channels = MemChannel
   
   TwitterAgent.sources.Twitter.consumerKey = xxxxxxxxxxxxxxxxxxxx
   TwitterAgent.sources.Twitter.consumerSecret = xxxxxxxxxxxxxxxxxxxx
   TwitterAgent.sources.Twitter.accessToken = xxxxxxxxxxxxxxxxxxxx
   TwitterAgent.sources.Twitter.accessTokenSecret = xxxxxxxxxxxxxxxxxxxx
   
   # Имя_нашего_агента.источники_данных.Twitter.ключевые_слова_по_нашей_теме_через_запятую
   TwitterAgent.sources.Twitter.keywords = 
   ```

7. Настраиваем сток (куда собираемся сливать данные). В нашем случае это будет HDFS. Указываем агент (TwitterAgent), через точку сток (sinks), имя стока (HDFS, имя задали в первом блоке)
   
   - .channel = MemChannel (канал откуда будем забирать данные)
   
   - .type = hdfs (указываем тип, hdfs - зарезераированное слово)
     
     - .path (путь куда сбрасываем данные)
     
     - .inUsePrefix =. (это значит, что файл, который только начал записываться имеет имя, начинающееся с точки, пока он не запишется)
     
     - .fileType = DataStream (тип файла, DataStream)
   
   ```textile
   # SINK HDFS
   
   # Имя_нашего_агента.стоки.HDFS.каналы_передачи_данных
   TwitterAgent.sinks.HDFS.channel = MemChannel
   
   # Имя_нашего_агента.стоки.HDFS.тип_стока
   TwitterAgent.sinks.HDFS.type = hdfs
   
   # Имя_нашего_агента.стоки.HDFS.путь_к_папке_назначения
   TwitterAgent.sinks.HDFS.hdfs.path = hdfs:///user/cloudera/tweets
   
   # Имя_нашего_агента.стоки.HDFS.префикс_для_временных_файлов
   TwitterAgent.sinks.HDFS.hdfs.inUsePrefix =.
   
   # Имя_нашего_агента.стоки.HDFS.формат_файла
   TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
   
   # Настройка частоты сброса и размер файла.
   TwitterAgent.sinks.HDFS.hdfs.batchSize = 10
   TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
   TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600
   TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
   ```

8. Подробнее о том, какие параметры можно настраивать при сборе данных можно посмотреть на сайте Flume. В т.ч. можно настраивать имена файлов - добавлять в них дату, время и т.д.

9. Последним этапом настраиваем канал (CHANNEL). Говорим, MemChannel (мы его так назвали в первом блоке) будет иметь тип memory, объем канала 100 сообщений.
   
   ```textile
   # CHANNEL
   
   # Имя_нашего_агента.каналы.MemChannel.тип_канала
   TwitterAgent.channels.MemChannel.type = memory
   
   # Имя_нашего_агента.каналы.MemChannel.объем канала
   TwitterAgent.channels.MemChannel.capacity = 100
   ```

10. Сохраняем. Первоначальная настройка Flume для сбора данных из Twitter завершена. Возвращаемся в основное меню Cloudera Manager. Видим дополнительный значек напротив Flume, который говорит, что конфигурация Flume устарела. Перезапускаем сервис с новой конфигурацией.

11. После перезапуска сервиса, Flume начал собирать данные из Twitter. Проверим это через Hue. Заходим в папку, которую мы указали для сбора логов (/user/cloudera/tweets).

12. Если файлы не появились начинаем с проверки кофигурации Flume. Через основное меню идем в  Flume, далее в Instance, в нашем случае запущен один агент на одном узле (может быть несколько и они могут быть запущены на разных узлах, более того на одном узле может быть запущено более одного агента). Переходим к Agent, далее Log Files -> Role Log File.

## Практическое задание 4 (Apache Kafka)

1. Создадим топик с одной партицией со степенью репликации 1 (т.к. у нас один узел в обучающем кластере). Запишем туда сообщение, после чего это сообщение прочтем. Далее прочтем все сообщения, которые есть в этом топике с самого начала и следующим шагом модифицируем конфигурацию Flume так, чтобы он начал писать еще и в Kafka. Т.е. мы будем собирвть архив данных в HDFS и при этом параллельно данные будут реплицироваться в Kafka для дальнейшего анализа. Посмотрим, что из твиттера пишется в реальном времени в Kafka.

2. Работать будем через консоль. Посмотрим, какие топики уже существуют. Это команда `kafka-topics` с параметром `--list`. Кроме того команда ждет от нас указания zookeeper (сервис для хранения конфигураций) и нам нужно знать к какому kafka-кластеру мы подключаемся. 2181 - стандартный порт для zookeeper.
   
   > `kafka-topics` # справка по команде
   > 
   > `kafka-topics --zookeeper quickstart.cloudera:2181/kafka --list` # просмотр топиков

3. Видим сообщения, но топиков нет.

4. Создадим топик tweets.
   
   > `kafka-topics --zookeeper quickstart.cloudera:2181/kafka --create --topics tweets --partition 1 --replication-factor 1 ` # создание топика

5. Ждем какое-то время и видим, что топик создан.

6. Снова проверяем список топиков, которые есть. Видим топик tweets.

7. Запишем что-нибудь в топик tweet с помощью команды `kafka-console-produser`. Этой утилите необходимо указать какой брокер отправляет сообщение, его порт будет 9020 (можно посмотреть в конфигурации), и указываем в какой топик писать (в наш tweets). Т.е. мы пишем в конкретный брокер, который находится на этом узле и пишем в топик tweets.
   
   > `kafka-console-produser --broker-list quickstart.cloudera:9092 --topic tweets ` # доступ к отправке сообщения в Kafka через консоль

8. Открывается приглашение к записи сообщения `>`, в этой строке пишем `hello world`, `hi big data`

9. Чтобы понять, что мы отправляем сообщения - открываем еще одно окно PuTTy, подключаемся к тому же топику и читаем сообщения.

10. Нажимаем правой копкой по PuTTy, выбираем duplicate session, логинимся в нем. Запускаем `kafka-console-consumer`, который позволяет читать из Kafka. Нужно указать kafka-кластер, каким брокером пишем в данном случа не важно и из какого топика читать.
    
    > `kafka-console-consumer --zookeeper quickstart.cloudera:2181/kafka --topic tweets` # чтение из топика kafka

11. Теперь при вводе сообщения в первом окне PuTTy они дублируются во втором окне.

12. Теперь нам бы хотелось прочитать и те сообщения, которые мы написали ранее. Для этого указываем опцию `-- from-beginning`
    
    > `kafka-console-consumer --zookeeper quickstart.cloudera:2181/kafka --topic tweets --from-beginning` # чтение всех сообщений из топика kafka

13. При этом, в этом режиме мы продолжаем слушать сообщения налету.

14. Внесем изменения в конфигурацию Flume:
    
    - ДОБАВЛЯЕМ ВТОРОЙ КАНАЛ, КОТОРЫЙ КОТОРЫЙ ОТПРАВЛЯЕТ ДАННЫЕ В KAFKA
    
    - ПОЯВЛЯЕТСЯ ВТОРОЙ СТОК Kafka
    
    - ДОБАВЛЯЕМ КАНАЛ KAFKA В TWITTER, ЧТОБЫ ТУДА ПОСТУПАЛИ ДАННЫЕ
    
    - ДОБАВЛЯЕМ КАНАЛ KAFKA
    
    - ДОБАВЛЯЕМ СТОК ДЛЯ KAFKA
    
    ```textile
    # vibo: откуда поступают данные (Twitter)
    # Имя_нашего_агента.источники_данных
    TwitterAgent.sources = Twitter
    
    # vibo: что мы будем использовать в качестве канала (канал в оперативной памяти)
    # Имя_нашего_агента.каналы_передачи_данных
    # ДОБАВЛЯЕМ ВТОРОЙ КАНАЛ, КОТОРЫЙ КОТОРЫЙ ОТПРАВЛЯЕТ ДАННЫЕ В KAFKA
    TwitterAgent.channels = MemChannel MemChannelKafka
    
    # vibo: куда будем сливать наши данные (в HDFS)
    # Имя_нашего_агента.стоки
    # ПОЯВЛЯЕТСЯ ВТОРОЙ СТОК Kafka
    TwitterAgent.sinks = HDFS Kafka
    
    ########## Source Twitter ##########
    
    # Имя_нашего_агента.источники_данных.Twitter.класс_отвечающий_за_источник
    TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSourse
    
    # Имя_нашего_агента.источники_данных.Twitter.название_канала
    # ДОБАВЛЯЕМ КАНАЛ KAFKA В TWITTER, ЧТОБЫ ТУДА ПОСТУПАЛИ ДАННЫЕ
    TwitterAgent.sources.Twitter.channels = MemChannel MemChannelKafka
    
    TwitterAgent.sources.Twitter.consumerKey = xxxxxxxxxxxxxxxxxxxx
    TwitterAgent.sources.Twitter.consumerSecret = xxxxxxxxxxxxxxxxxxxx
    TwitterAgent.sources.Twitter.accessToken = xxxxxxxxxxxxxxxxxxxx
    TwitterAgent.sources.Twitter.accessTokenSecret = xxxxxxxxxxxxxxxxxxxx
    
    # Имя_нашего_агента.источники_данных.Twitter.ключевые_слова_по_нашей_теме_через_запятую
    TwitterAgent.sources.Twitter.keywords = 
    
    ########## SINK HDFS ##########
    
    # Имя_нашего_агента.стоки.HDFS.каналы_передачи_данных
    TwitterAgent.sinks.HDFS.channel = MemChannel
    
    # Имя_нашего_агента.стоки.HDFS.тип_стока
    TwitterAgent.sinks.HDFS.type = hdfs
    
    # Имя_нашего_агента.стоки.HDFS.путь_к_папке_назначения
    TwitterAgent.sinks.HDFS.hdfs.path = hdfs:///user/cloudera/tweets
    
    # Имя_нашего_агента.стоки.HDFS.префикс_для_временных_файлов
    TwitterAgent.sinks.HDFS.hdfs.inUsePrefix =.
    
    # Имя_нашего_агента.стоки.HDFS.формат_файла
    TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
    
    # Настройка частоты сброса и размер файла.
    TwitterAgent.sinks.HDFS.hdfs.batchSize = 10
    TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
    TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600
    TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
    
    ########## CHANNEL ##########
    
    # Имя_нашего_агента.каналы.MemChannel.тип_канала
    TwitterAgent.channels.MemChannel.type = memory
    
    # Имя_нашего_агента.каналы.MemChannel.объем канала
    TwitterAgent.channels.MemChannel.capacity = 100
    
    # ДОБАВЛЯЕМ КАНАЛ KAFKA
    
    ########## ADDED KAFKA CHANNEL ##########
    TwitterAgent.channels.MemChannelKafka.type = memory
    TwitterAgent.channels.MemChannelKafka.capacity = 100
    
    # ДОБАВЛЯЕМ СТОК ДЛЯ KAFKA
    
    ########## SINK KAFKA ##########
    # vibo: стандартный sink для flume, может сразу писать в kafka
    TwitterAgent.sinks.Kafka.type = org.apache.flume.sink.kafka.KafkaSink
    # vibo: указываем наименование топика
    TwitterAgent.sinks.Kafka.topic = tweets
    # vibo: указываем брокера, который будем писать
    TwitterAgent.sinks.Kafka.brokerList = quickstart.cloudera:9092
    # vibo: указываем канал, через который это делать
    TwitterAgent.sinks.Kafka.chennel = MemChannelKafka
    # vibo: и указываем сколько сообщений
    TwitterAgent.sinks.Kafka.batchSize = 20
    ```

15. Обновляем конфигурацию в Flume (Configuration File). Сохраняем изменения. Рестартуем кофигурацию Flume. Проверяем, пишутся ли сообщения к Kafka (PuTTy, второй терминал).

## Практическое задание 5 (Hive и Impala)

1. Нужно создать таблицу над теми данными, которые мы получаем из твиттера, которые сейчас лежат в HDFS в виде json-файлов. Провести их обработку. В Hive будем считать, сколько сообщений лежит в каждом файле и сколько всего сообщений успело накопиться в нашем архиве. В Impala мы посчитаем количество строк, но посмотрим разницу между двумя этими системами, как они работают с данными, которые постоянно поступают.

2. Начинаем с Hive. Чтобы создать в нем таблицу переходим в Hue, выбираем Editors, во вкладке Documents выбираем Hive. Сразу попадаем в базу данных по умолчанию (называется default). Можем в нее зайти, здесь пока не создано ни одной таблицы.

3. Создать таблицу можно вручную, или используя sql-синтаксис. Воспользуемся sql, создадим таблицу SRC_TWEETS с одним столбцом full_tweet, формат String. Далее говорим, как у нас хранятся эти данные (TEXTFILE), если бы они лежали в каким-то формате мы бы написали STORED AS PARQUET, STORED AS SRC. Последнее, что нужно указать location, т.е. в какой папке лежат эти данные (можно с префиксом hdfs, можно без него, hive поймет, что файлы на распределенной файловой системе). В этом примере мы создали таблицу **непрерывно связанную с данными**. Если потребуется удалить таблицу, то и данные удалятся вместе ней.
   
   ```sql
   CREATE TABLE IF NOT EXISTS SRC_TWEETS (full_tweet String)
   STORED AS TEXTFILE
   LOCATION '/user/cloudera/tweets';
   ```

4. Добавляем **EXTERNAL (подобие view, которая смотрит на данные)** - отвязываем метаданные от самих данных и теперь при удалении таблицы (т.е. ее метаданных) удалится только ее описание, а не сами данные; они как лежали в HDFS, так и останутся, можн овсегда создать еще и еще разные таблицы.
   
   ```sql
   CREATE EXTERNAL TABLE IF NOT EXISTS SRC_TWEETS (full_tweet String)
   STORED AS TEXTFILE
   LOCATION '/user/cloudera/tweets';
   ```

5. Нажимаем на треугольник для создания таблицы. Чтобы ее увидеть слева нажимаем refresh.

6. Посмотрим, что в ней лежит с ограничением в 10 записей. Видим, что каждый твит уже лежит отдельно в своей ячейке.
   
   ```sql
   SELECT * from src_tweets LIMIT 10;
   ```

7. Посмотрим сколько твитов в каждом файле. Для этого у Hive есть специальная системная колонка (INPUT__FILE__NAME), которая позволяет взять имя файла из которого мы берем информацию. Считаем сколько строк и группируем по каждому файлу. Таким образом, узнаем сколько твитов лежит в каждом файле. После запуска выполяется MapReduce-программа (пиктограмма с самолетом обозначает, что программа еще выполняется). В правом верхнем углу можно вывести логи. Видим, что запустилась job, также есть ссылка, чтобы посмотреть подробности. Также здесь указан % выполнения map и reduce задачи.
   
   ```sql
   SELECT INPUT__FILE__NAME, count(full_tweet) FROM src_tweets
   GROUP BY INPUT__FILE__NAME;
   ```

8. Простой запрос с группировкой занимает значительное время. Посмотрим потом как это можно ускорить в Impala. А здесь движок MapReduce.

9. Посмотрим сколько всего строк собралось (сколько твитов). Получили порядка 56 тыс. твитов.
   
   ```sql
   SELECT count(full_tweet) FROM src_tweets;
   ```

10. Как мы видим запросы в Hive выполняются долго. Для аналитики это не очень подходит, для формирования отчетов вполне. При этом сейчас наши данные хранятся не оптимально, простым текстом, без всякого сжатия, это не поколоночное хранение. Мы могли бы положить данные в CSV, любой другой файл и выполнять над ним sql. Мы могли бы рассказать как разбить json на ячейки и писать запрос к конкретным колонкам.

11. Попробуем сделать все тоже самое через Impala. Переходим в другой Editor -> Impala. У нас единая база данных default для Hive и Impala. Но табличку на текущий момент мы не видим. Чтобы ее увидеть нам нужно синхронизировать метаданные Impala c Hive. Impala берет эти данные на определенный момент времени и не знает, что они меняются в Hive. Нажимаем на значек refresh и выбираем `Perform incremental metadata update` или `Invalidate all metadata and rebuild index`(все заново выкачивает). После этого дложна появиться табличка src_tweets в Impala.

12. Собираем статистику, чтобы запросы выполнялись быстрее. COMPUTE STATS позволяет просканировать таблицу и построить по ней индексы.
    
    ```sql
    COMPUTE STATS src_tweets;
    ```

13. Теперь делаем запрос count() в Impala. Запрос выполняется практически мгновенно. Теже 56200 твитов.
    
    ```sql
    SELECT count(full_tweet) FROM src_tweets;
    ```

14. Переходим обратно в Hive, делаем снова запрос - количество твитов за это время увеличилось. Возращаемся в Impala, делаем запрос без обновления таблицы - значение твитов не изменилось - 56200. Hive медленно, но выдает актуальную информацию. Impala - выдает статистику по срезу на определенный момент и показывает его.

## Практическое задание 6 (Apache Spark)

1. Выполним wordcount, используя Spark и Python. После чего те данные, которые у нас грузились в Kafka мы будем потоково в Spark получать, структурировать и сохранять в таблицу в формате паркет. Проверим, что данные появились и посчитаем количество друзей у каждого пользователя. Работать со Spark будем через консоль.

2. Открываем Putty, подключаемся. Выполняем команду `pyspark2`
   
   > `pyspark2` # запуск спарка для работы через интерактивную консоль

3. Будем работать с последней версией спарка 2.0. Мы запустили приложение через консоль, посмотрим, как это выглядит в Cloudera Manager на кластере. Переходим во вкладку Clusters -> YARN Applications. Видим, что теперь у нас есть PySparkShell (ему присоен номер), пока мы работаем в консоли, на кластере выполняется приложение.

4. Данные для подсчета количества слов будем брать также из дирректории /user/cloudera/example_tweets, файл example.json. Прочитаем этот файл в переменную spark (sc - spark context). Напомним, что Spark выполнит запрос, когда мы его попросим материализовать данные.
   
   > `text_file = sc.textFile("hdfs:///user/cloudera/example_tweets")`

5. Попросим спарк показать, что что-то произошло. Для этого возьмем первую строчку из файла text_file:
   
   > `text_file.take(1)` # в этот момент происходит выполнение запроса

6. Если посмотреть во вкладке Clusters -> YARN Applications -> PySparkShell -> application переходим во вкладку Hadoop, здесь нажимаем на ApplicationMaster, попадаем в интерфейс Spark, а не как раньше в MapReduce задание. В логах можно увидеть, что Python вызывает для работы Scala.

7. Во вкладке Executors можно посмотреть кто выполнял работу: есть один driver и executor, который и выполнял работу.

8. Выделим слова, разбиваем каждую строку по пробелам. Берем переменную `text_file`, вызываем функцию `flatMap` (считает количество слов в одной строке). Говорим, что на входе у нас будет строка, а на выходе - строка разбитая через пробел (split)
   
   > `words = text_file.flatMap(lambda line: line.split(" "))`
   > 
   > `words.take(1)`

9. На вкладке Spark можем посмотреть DAG Visualization.

10. Создаем переменную word_count, точнее пару, которая состоит из слова и переменной 1 (пара ключ - значение)
    
    > `words_count = words.map(lambda word: (word, 1))`

11. Далее считаем с помощью команды `reduceByKey`, которая возьмет на вход все наши одинаковые слова и сложит 1. Сохраним результат.
    
    > `counts = words_count.reduceByKey(lambda a,b: a+b)`
    > 
    > `counts.saveAsTextFile("hdfs:///user/cloudera/spark_wc_result")`

12. Чтобы посмотреть результат идем в Hue, папка `spark_wc_result`. Получили 8 файлов, т.к. у нас было 8 executors. Видим, что пример на Spark занимает намного меньше места, чем на java. Мы не брали готовую библиотеку, написали находу.

13. Во второй части подключаемся к Kafka и пробуем выкачать из нее твиты. Снова запускаем pyspark2. Нам потребуется импортировать ряд библиотек, чтобы подключаться к Kafka, они идут в комплекте spark и т.к. твиты идут постоянно, то мы будем использовать SparkStreaming, который позволяет работать с потоком данных. Подключим модуль для работы с потоками данных, модуль по работе с Kafka. Импортируем библиотеку `json`, которая может конвертировать текст json в объект и обратно.
    
    > `from pyspark.streaming import StreamingContext`
    > 
    > `from pyspark.streaming.kafka import KafkaUtils`
    > 
    > `import json`

14. Подключаемся к SparkStreaming, и обновляем данные каждые 60 секунд. Т.е. 60 секунд накапливаем данные, потом начинаем их обрабатывать.
    
    > `streamingContext = StreamingContext(spark.sparkContext, 60)`

15. Подключаемся к Kafka, используя созданный выше streamingContext, указываем топик, который будем смотреть (tweets), также говорим куда подключиться - указываем список брокеров.
    
    > `directKafkaStream = KafkaUtils.createDirectStream(stremingContext, ["tweets"], {"metadata.broker.list": "quickstart.cloudera:9092"})`

16. Для накопленной информации за 60 секунд выполним преобразование - выдели твиты; directKafkaStream - это будет набор данных RDD, которые к нам вернутся. Для накопившихся данных за 60 секунд мы выполним функцию map, которая на вход получит одно сообщение из Kafka и возьмем из него только первый объект. Через библиотеку json делаем объект, его и возвращаем.
    
    > `tweets = directKafkaStream.map(lambda v: json.loads(v[1]))`
    > 
    > `tweets.pprint()` # вывод первых 10 записей
    > 
    > `streamingContext.start()` # запуск Streaming

17. Останавливаем выполнение.

18. Теперь мы хотим сохранить информацию в виде таблицы для обработки Impala. Сначала сохраним таблицу в Hive, потом перейдем в Impala. У нас уже есть заготовка, которая подключается к Kafka и начинает из нее выкачивать данные. Данные возвращаются в качестве объектов - твитов, нам необходимо из них сделать таблицу. Возьмем готовую схему - /user/cloudera/schema_example.json. В данном файле наиболее полно описана схема из документации твиттера, что он возвращает и тип данных.

19. Запускаем заново spark2, импортируем библиотеки StreamingContext, KafkaUtils, json. Возьмем пример tweet_example. Прочитаем его, получмим из него табличку. Используя spark session прочтем этот файл в формате json и сразу сделаем из него табличное представление.
    
    > `tweet_example = spark.read.load("hdfs:///user/cloudera/schema_example.json", "json")`

20. Скажем как будет называться наша таблица и укажем путь
    
    > `db_table = "default.tweets"`
    > 
    > `table_path = "hdfs:///user/cloudera/streamed-tweets-parquet"`

21. Далее нам нужно понять существует ли наша таблица. Если существует, то нужно дописывать данные, если не существует ее нужно создать. spark.catalog.listTables() - позволяет посмотреть какие таблицы у нас есть, проходя по нему циклом мы сравниваем названия таблицы с нашим названием и если оно есть, то в listTables что-то появится.
    
    > table_exists = [table for table in spark.catalog.listTables() if table.database+' . '+table.name == db table] 

22. Напечатаем table_exists. Видим [] - папка пустая
    
    > `table_exists`

23. Теперь запишем, если наша таблица не существует - создаем пустой датафрейм и берем схему данных из примера твитта. Создаем таким образом таблицу.
    
    > `if not table exists:`
    > 
    > `df = spark.createDataFrame(sc.emptyRDD(), tweet_example.schema`
    > 
    > `df.write.saveASTable(db_table, format='parquet', mode='overwrite', path='table_path')`

24. Проверяем через Hue, создалась ли таблица. Переходим в Hive, обновляем. Проверяем наличие таблицы tweets. При нажатии на нее видим ее структуру, из примера твита. Посмотрим что-нибудь (должно быть пусто):
    
    ```sql
    SELECT * FROM tweets;
    ```

25. Создадим функцию, которая будет записывать данные из Kafka в эту таблицу (пишем в PuTTy). Принимает на вход RDD, если RDD пустой, то ничего не делаем, а если не пустой - то мы из него создаем датафрейм, используя схему примера и дописываем в нашу табличку. В конце возращаем RDD.
    
    ```python
    def saveToTable(rdd):
        try:
            # Проверяем, что RDD не пустой
            if not rdd.isEmpty():
                # Создаем DataFrame. В качестве данных - RDD, в качестве схемы - схем примера
                df = spark.creatDataFrame(rdd, tweet_example.schema)
                # Дописываем в таблицу defualt.tweets
                df.write.mode("append").insertInto(db_table)
        except Exception as e:
            print("Ooops!", e)
        return rdd
    ```

26. Возвращаемся в Kafka и вызываем для каждого набора данных нашу функцию.
    
    > `streamingContext = StreamingContext(spark.sparkContext, 60)`
    > 
    > `directKafkaStream = KafkaUtils.createDirectStream(stremingContext, ["tweets"], {"metadata.broker.list": "quickstart.cloudera:9092"})`
    > 
    > `tweets = directKafkaStream.map(lambda v: json.loads(v[1]))`
    > 
    > `tweets.foreachRDD(lambda x: saveToTable(x))`
    > 
    > `streamingContext.start()` # запуск Streaming
    > 
    > `streamingContext.awaitTermination()` # ждем прерывания

27. Идем в Cloudera Manager, YARN -> Applications -> Spark -> Streaming. Первая итеррация завершена, можем посмотреть наполнение таблицы через Hue в Hive:
    
    ```sql
    SELECT * FROM tweets;
    ```

28. Видим, что таблица уже не пустая. Данные вставляются благодаря приложению на спарке, которое мы написали. Промышленные приложения запускаются не так. Мы посмотрели скорее на способ отладить наше приложение, посмотреть как оно работает. Для запуска мы все, что было написано оборачиваем в питоновский файл и запускаем через spark submit, направляем на кластер, там же создается драйвер и выполняется этот код.

29. Перейдем из Hive в Impala. Найдем среднее количество друзей. Обновляем таблички (`Perform incremental metadata update`). Ждем пока в Impala появится таблчка Tweets. Посмотрим, что импортировалось (по хорошему перед этим нужно было посчитать статистику `COMPUTE STATS tweets` ):
    
    ```sql
    SELECT * FROM tweets;
    ```

30. User представляет собой структуру, состоящую из нескольких атрибутов, у него есть поле friends_count. Выведем имена пользователей и их количество друзей.
    
    ```sql
    SELECT user.name, user.friends_count FROM tweets;
    ```

31. Найдем среднее количество друзей, которые интересуются нашей темой (настройка - ключевые слова), в среднем 2000 друзей.
    
    ```sql
    SELECT avg(user.friends_count) as friends_avg FROM tweets;
    ```

## Практическое задание 7 (Solr)

1. Знакомимся с инструментом Solr (поисковой движок). Мы создадим специальную конфигурацию для хранения твитов, после чего зададим ее в Solr и попытаемся туда записать какие-то тестовые данные. Посмотрим, что они записались, поищем по ним. Перейдем в графический интерфейс Solr. Смотрим, какие у нас есть Instance, переходим в `Solr Server Web UI`. Видмим, что у нас нет ни одного ядра, их нужно создать. Делать это будем через консоль. Команда solrctl позволяет работать с solr, создавать так называемые ядра.
   
   > `solrctl` # вывод подсказки

2. В самом начале будем работать с `instancedir`, т.е. создадим специальную папку (tweets_conf_test), в которую сгенерируем нашу конфиграцию, базовую конфигурацию, которую потом поменяем и уже на основе нее создадим наше поисковое ядро, которое будет работать в соответствии с данной конфигурацией работать. `-schemaless` - схема будет подстраиваться под данные (будет плавающая). Далее создаем конфигурацию tweets, используя папку tweets_conf_test. Это значит, что мы загружаем конфигурацию в zookeeper для того, чтобы она была доступна нескольким серверам.
   
   > `solrctl instancedir --generate tweets_conf_test -schemaless`
   > 
   > `solrctl instancedir --create tweets tweets_conf_test/`

3. Посмотрим на конфигурацию, идем в папку tweets_conf_test/conf/ нас интересует schema.xml и solrconfig.xml. Еще есть файлы со стоп-словами, синонимами, все то, что может нам улучшить обработку наших слов.
   
   > `nano schema.xml` # просмотр содержимого

4. schema.xml - это файл с описанием, который говорит нам как разбирать слова, как их парсить.

5. Создадим коллекцию для твитов (-zk это zookeeper). Говорим адрес текущей машины, конфигурацию которую будем использовать.
   
   > `solrctl --zk quickstart:2181/solr collection --create tweets -c tweets`

6. Возвращаемся в графический интерфейс Solr, видим, что у нас создалось ядро  tweets, состоящее из одного shard, который находится на машине quickstart.cloudera.

7. Запишем пример данных в Solr, будем использовать питон. На виртуальной машине установлена анаконда, можно вызвать python 3.6. Импортируем библиотеку pysolr, с помощью которой будем загружать туда данные.
   
   > `/opt/anaconda/bin/python`
   > 
   > `import pysolr`

8. Подключаемся к zookeeper, указываем адрес. Подключаем нашу коллекцию - tweets_collection, используем SolrCloud и говорим, что наша коллекция называется tweets.
   
   > `zookeeper = pysolr.ZooKeeper("quickstart.cloudera:2181/solr")`
   > 
   > `tweets_collection = pysolr.SolrCloud(zookeeper, "tweets")`

9. Добавляем набор записей, которые имеют id и имя
   
   > `tweets_collection.add([("id":1,"name":"test1"), ("id":2,"name":"test2"),  ("id":3,"name":"test3")])`

10. Переходим в графический интерфейс, видим, что у нас появилось три новых документа. Можно зайти в Query, выполнить запрос и увидеть наши данные. Такой интерфейс не очень удобный, переходим в Hue. Переходим в Dashbord, где можем вывести таблицу. Построить график. Можно фильтровать данные просто нажимая на сектор диаграммы.

## Практическое задание 8
