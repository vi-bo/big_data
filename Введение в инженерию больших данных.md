НИТУ "МИСиС"

# Введение в инженерию больших данных

https://openedu.ru/

**О НИТУ "МИСиС"**

- [x] Думай ярко!

- [x] Общая информация

- [x] Цифры и факты

**Информация для слушателей**

- [x] Информация для слушателей

- [x] Техническая поддержка

- [x] Обсуждение

**Раздел 1. Что такое Big Data**

- [x] 1.1 Знакомство с технологиями Big Data

- [x] 1.2 Что такое Big Data + возможности

**Раздел 1. Что такое Big Data ч.2**

- [x] 1.3 Что такое Big Data - Сложности

- [ ] Контрольный тест 1 (до 20 дек. 2022 г., 23:55) ✏️

**Раздел 2. Ключевые технологии и подходы**

- [x] 2.1 Ключевые технологии и подходы

- [x] 2.2 Ключевые технологии и подходы

**Раздел 2. Ключевые технологии и подходы ч.2**

- [ ] 2.3 Ключевые технологии и подходы

- [ ] Контрольный тест 2 (до 20 дек. 2022 г., 23:55) ✏️

**Раздел 3. Инструменты получения данных**

- [ ] 3.1 Инструменты получения данных

- [ ] 3.2 А что если данные лежат в базе данных?

**Раздел 3. Инструменты получения данных ч.2**

- [ ] 3.3 Apache Flume и Apache Kafka

- [ ] Контрольный тест 3 (до 20 дек. 2022 г., 23:55) ✏️

**Раздел 4. Обработка данных**

- [ ] 4.1 Обработка данных в Apache Spark

- [ ] 4.2 Обработка данных

- [ ] 4.3 Обработка данных

**Раздел 4. Обработка данных ч.2**

- [ ] 4.4 Обработка данных

- [ ] 4.5 Apache Hive

- [ ] 4.6 Cloudera Impala

- [ ] Контрольный тест 4 (до 20 дек. 2022 г., 23:55) ✏️

**Раздел 5. Хранение данных**

- [ ] 5.1 Хранение данных

**Раздел 5. Хранение данных ч.2**

- [ ] 5.2 Apache Solr

- [ ] Контрольный тест 5 (до 20 дек. 2022 г., 23:55) ✏️

**Практическое задание**

- [ ] Практическое задание

## Раздел 1. Что такое Big Data

### 1.1 Знакомство с технологиями Big Data

#### Цели курса

- Определение термина "Большие данные"

- Знакомство с системой Apache Hadoop. Изучение соновных компонентов.

- Обзор основных компонентов экосистемы Hadoop
  
  - получение данных: Flume, Sqoop, Kafka
  
  - анализ и приобразование данных: Spark, MapReduce, Hive, Impala
  
  - хранение данных: HDFS, HBase, Solr
  
  - визуализация данных: Hue

- Сквозная практическая часть - анализ потоковых данных из Twitter

### 1.2 Что такое Big Data + возможности

#### Возможности

- здравоохранение (мониторинг состояния здоровья пациента )

![](/home/vibo/Pictures/GlobalMarkText/2022-09-22-22-36-42-image.png)

- промышленность (диагностика оборудования)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-42-10-image.png" title="" alt="" data-align="center">

#### Характеристики Big Data (4V)

- Volume - Объем

- Velocity - Скорость генерации

- Variety - Множество источников

- Value - Значимость

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-44-34-image.png" title="" alt="" data-align="center">

## Раздел 1. Что такое Big Data ч.2

### 1.3 Что такое Big Data - Сложности

#### Сложности

- Структура данных (как хранить, как собрать воедино, новые типы оборудования)

- Анализ (какие данные нужно собрать, дает или не дает полезную информацию показатель)

- Обработка (комментарии в социальных сетях, как анализировать)

- Управление (кейс с самолетом, большой набор критической информации, о предоставлении которой нужно договориться)

- Безопасность (много критической информации в одном месте, нужно разграничивать доступ к разным разделам)

#### Отличие от традиционных подходов

##### Традиционная "Схема на запись"

- качество данных (Data Quality) проверяется формаизованными ETL (extract-transform-load) процессами

- данные хранятся в табличной, согласованной целостной форме

- интеграция данных через ETL

- перед записью определить структуры хранения

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-58-15-image.png" title="" alt="" data-align="center">

##### Big Data "Схема на чтение"

-  данные интерпретируются каждой программой, получающей доступ к данным

- качество данных определяется качеством программы

- интеграция данных в программе <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-01-09-image.png" title="" alt="" data-align="center">

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-02-26-image.png" title="" alt="" data-align="center">

##### ИТ ландшафт

- Обработка событий (Stream processing)

- Резервуар данных (Data Lake)
  
  - Исторические данные (Cold Data)
  
  - Неструктурированные данные (Unstructed Data)
  
  - Исследование данных (Data Discovery)

- Корпоративное хранилище (DWH)

- Отчетность (BI)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-04-12-image.png" title="" alt="" data-align="center">

##### Технологии и инструменты

* Hadoop & MapReduce (стандарт больших данных)

* NoSQL базы данных (в которых нет SQL доступа, они осуществляют хранение, но хранят их в кластере - распределенной системе и позволяют осуществить доступ к данным быстро, чаще всего по ключу)

* Углубленная аналитика
  
  - статистика
  
  - предиктивная аналитика и data mining
  
  - лингвистичесткая обработка текстов
- инструментыкласса Data Discovery (для бизнесс аналитики, не сильно погружаясь в языки программирования строить дашборды)

## Раздел 2. Ключевые технологии и подходы

### 2.1 Ключевые технологии и подходы

#### Распределенные вычисления

**Распределенные вычисления** - метод который позволяет отдельным компьютерам работать над одной задачей через сеть (увеличивая скорость).

**Распределенная файловая система** - "клиент-серверное" приложение, которое позволяет получать доступ и обрабатывать данные, находящиеся на удаленных серверах так, как будто они находятся на локальном компьютере. Файловые системы, упраляющие хранением и распределением данных в сети, называются распредленными файловыми системами. Например, DropBox и прочие.

#### Компьютерный кластер

**Компьютерный кластер** - единый логический блок, состоящий из нескольких компьютеров (racks), которые соединены по локальной сети (LAN).

**Компоненты кластера** - узлы (компьютеры), используемые в качестве серверов, имеющие свою собственную операционную систему.

**Узел (Node)** - компьютер, обычно включает в себя ЦП, память и дисковую подсистему.

#### Apache Hadoop

Apache Hadoop является программной платформой с открытым исходным кодом дял распредлеенного хранения и обработки больши объемов данных.

##### Особенности

- открытый исходный код

- распределенная файловая система HDFS

- неограниченные объемы данных

- HDFS является надстройкой над обычной файловой системой, например, такой, как Linux

- реализация парадигмы распределенных вычислений MapReduce

##### Виды аналитики, используемые в Hadoop

- текстовый анализ

- построение индексов

- построение и анализ графов 

- распознавание образов

- совместная фильтрация (Collaborative filtering)

- создание прогнозных моделей

- анализ тональности

- оценка рисков

#### Основные компоненты Apache Hadoop

##### Apache Hadoop Distributed File System (HDFS)

- архитектура "Master-Slave"

- построена на основе Google's File System (GFS)

- способна сохранять распределенные данные на разных узлах кластера

- репликация данных (данные копируются на несколько узлов)

- отказоустойчивость и высокая доступность

- поддержка MapReduce (распределнная и параллельная обработка)

- масштабируемость

### 2.2 Ключевые технологии и подходы

#### Ключевые определения HDFS

**Blocks** - файлы разбиваются на блоки (64 Mb по умолчанию), блоки хранятся в разных узлах.

**Replication Factor** - степень репликации каждого блока в файловой системе (по умолчанию - 3).

**NameNode (NN)** - сервис, который хранит метаданные обо всех файлах и дирректориях в файловой системе.

**Secondary NameNode** - периодически выполняет применение накопившихся изменений, сокращая время старта сервиса NameNode (сохраняет последнее состояние snapshot).

**DataNode (DN)** - хранит данные в блоках.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-56-30-image.png" title="" alt="" data-align="center">

#### HDFS High Availibility (HA)

HDFS High Availibility (HA) - функция высокой доступности HDFS, запускаются два NameNode: Activity и Standby, позволяя быстро переключаться между ними в случае отказа одного.

## Раздел 2. Ключевые технологии и подходы ч.2

### 2.3 Ключевые технологии и подходы

## Раздел 3. Инструменты получения данных

### 3.1 Инструменты получения данных

### 3.2 А что если данные лежат в базе данных?

## Раздел 3. Инструменты получения данных ч.2

### 3.3 Apache Flume и Apache Kafka

## Раздел 4. Обработка данных

## Раздел 4. Обработка данных ч.2

### 4.1 Обработка данных в Apache Spark

### 4.2 Обработка данных

### 4.3 Обработка данных

#### Resilient Distributed Dataset (RDD)

- **Resilient:**
  
  - Если сломается узел, который выполняет часть Spark приложения, набор данных, который хранился ну узле может быть восстановлен на другом узле.

- **Distibuted:**
  
  - Данные в RDD разделены на несколько партиций и распределены между узлами кластера.

- **Data set:**
  
  - RDD - набор данных, который состоит из записей (строк).
  
  - Записи представляют собой уникально идентифицируемые коллекции в наборе данных.
  
  - Записи могут представлять собой набор полей, похожих на строки в таблице, строку текста в файле или некоторые сложные объекты.

#### Терминология RDD

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-25-15-image.png" title="" alt="" data-align="center">

#### Операции RDD

##### Трансформации:

- Связывание (Map)

- Фильтрация (Filter)

- Выборки (Sample)

- Объединение (Union)

- Группирование по ключу (Groupbykey)

- Свертка по ключу (Reducebykey)

- Соединение и кеширование (Join & cache)

Трансформации в Spark осуществляются в "ленивом" режиме - результат не вычисляется сразу после трансформации. Вместо этого они "запоминают" операцию, которую следует произвести, и набор данных, над которым нужно совершить операцию. Вычисление трансформаций происходит только тогда, когда вызывается действие.

##### Действия:

- Свертка (Reduce)

- Сбор (Collect)

- Подсчет (Count)

- Сохранение (Save)

- Поиск (Lookpkey)

#### Пример RDD

![](/home/vibo/Pictures/GlobalMarkText/2022-09-22-22-25-46-image.png)

#### WordCount on PySpark

- Читаем файл

```python
text_file = sc.textFile("hdfs://...")
```

- Делим по пробелам, присваиваем каждому слову 1, группируем по словам и складываем числа

```python
counts = text_file.flatMap(lambda line: line.split(" ")) \
              .map(lambda word: (word, 1)) \
              .reduceByKey(lambda a,b: a + b)
```

* Сохраняем

```python
counts.saveAsTextFile("hdfs://...")
```

### 4.4 Обработка данных

#### Spark Streaming

Модуль Spark Streaming работает с потоковыми данными. Spark разбивает потоки данных на RDD и позволяет выполнять те же действия, что и с обычными данными.

- Простота использования (пишем потоковые программы не сильно меняя код)

- Отказоустойчивость (перезапускает приложение на другом узле, если упало)

- Унификация (streaming, batch, interactive)

- Поддержка машинного обучения и SQL

- Оперирует Dstream - Discretized stream <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-19-50-image.png" title="" alt="" data-align="center">

#### WordCount

Смотрим, как запустить предыдущий пример в стриминге и постоянно высчитывать количество слов, которые идут в нашем потоке.

```python
pairs = words.map(lambda word: (word, 1))
# считаем сколько слов в каждой порции
wordCounts = pairs.reduceByKey(lambda x, y: x + y)
# выводим информацию на экран
wordCounts.pprint()
# запуск вычислений
ssc.start()
# ждем завершения
ssc.awaitTermination()
```

#### DStream

- Dstream - поток данных RDD, основной уровень абстракции для Spark Streaming

- Преимущества:
  
  - балансировка нагрузки (можно распределять данные между разными узлами)
  
  - быстрое востановление после сбоев (если какой-то сервер упалЮ то все данные, которые на него шли распределяются между другими серверами)
  
  - унификация (код, который писали для обработки набора данных, батча - можно использовать в стриминге)

##### Балансировка нагрузки

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-15-59-image.png" title="" alt="" data-align="left">

##### Быстрое восстановление после сбоев

![](/home/vibo/Pictures/GlobalMarkText/2022-09-22-22-16-29-image.png)

### 4.5 Apache Hive

### 4.6 Cloudera Impala

## Раздел 5. Хранение данных

### 5.1 Хранение данных

## Раздел 5. Хранение данных ч.2

### 5.2 Apache Solr

## Практическое задание
