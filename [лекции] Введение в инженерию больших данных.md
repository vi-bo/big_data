НИТУ "МИСиС"

# Введение в инженерию больших данных

https://openedu.ru/

## Раздел 1. Что такое Big Data

### 1.1 Знакомство с технологиями Big Data

#### Цели курса

- Определение термина "Большие данные"

- Знакомство с системой Apache Hadoop. Изучение соновных компонентов.

- Обзор основных компонентов экосистемы Hadoop
  
  - получение данных: Flume, Sqoop, Kafka
  
  - анализ и приобразование данных: Spark, MapReduce, Hive, Impala
  
  - хранение данных: HDFS, HBase, Solr
  
  - визуализация данных: Hue

- Сквозная практическая часть - анализ потоковых данных из Twitter

### 1.2 Что такое Big Data + возможности

#### Возможности

- здравоохранение (мониторинг состояния здоровья пациента )

![](/home/vibo/Pictures/GlobalMarkText/2022-09-22-22-36-42-image.png)

- промышленность (диагностика оборудования)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-42-10-image.png" title="" alt="" data-align="center">

#### Характеристики Big Data (4V)

- Volume - Объем

- Velocity - Скорость генерации

- Variety - Множество источников

- Value - Значимость

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-44-34-image.png" title="" alt="" data-align="center">

## Раздел 1. Что такое Big Data ч.2

### 1.3 Что такое Big Data - Сложности

#### Сложности

- Структура данных (как хранить, как собрать воедино, новые типы оборудования)

- Анализ (какие данные нужно собрать, дает или не дает полезную информацию показатель)

- Обработка (комментарии в социальных сетях, как анализировать)

- Управление (кейс с самолетом, большой набор критической информации, о предоставлении которой нужно договориться)

- Безопасность (много критической информации в одном месте, нужно разграничивать доступ к разным разделам)

#### Отличие от традиционных подходов

##### Традиционная "Схема на запись"

- качество данных (Data Quality) проверяется формаизованными ETL (extract-transform-load) процессами

- данные хранятся в табличной, согласованной целостной форме

- интеграция данных через ETL

- перед записью определить структуры хранения

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-58-15-image.png" title="" alt="" data-align="center">

##### Big Data "Схема на чтение"

-  данные интерпретируются каждой программой, получающей доступ к данным

- качество данных определяется качеством программы

- интеграция данных в программе <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-01-09-image.png" title="" alt="" data-align="center">

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-02-26-image.png" title="" alt="" data-align="center">

##### ИТ ландшафт

- Обработка событий (Stream processing)

- Резервуар данных (Data Lake)
  
  - Исторические данные (Cold Data)
  
  - Неструктурированные данные (Unstructed Data)
  
  - Исследование данных (Data Discovery)

- Корпоративное хранилище (DWH)

- Отчетность (BI)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-04-12-image.png" title="" alt="" data-align="center">

##### Технологии и инструменты

* Hadoop & MapReduce (стандарт больших данных)

* NoSQL базы данных (в которых нет SQL доступа, они осуществляют хранение, но хранят их в кластере - распределенной системе и позволяют осуществить доступ к данным быстро, чаще всего по ключу)

* Углубленная аналитика
  
  - статистика
  
  - предиктивная аналитика и data mining
  
  - лингвистичесткая обработка текстов
- инструментыкласса Data Discovery (для бизнесс аналитики, не сильно погружаясь в языки программирования строить дашборды)

## Раздел 2. Ключевые технологии и подходы

### 2.1 Ключевые технологии и подходы

#### Распределенные вычисления

**Распределенные вычисления** - метод который позволяет отдельным компьютерам работать над одной задачей через сеть (увеличивая скорость).

**Распределенная файловая система** - "клиент-серверное" приложение, которое позволяет получать доступ и обрабатывать данные, находящиеся на удаленных серверах так, как будто они находятся на локальном компьютере. Файловые системы, упраляющие хранением и распределением данных в сети, называются распредленными файловыми системами. Например, DropBox и прочие.

#### Компьютерный кластер

**Компьютерный кластер** - единый логический блок, состоящий из нескольких компьютеров (racks), которые соединены по локальной сети (LAN).

**Компоненты кластера** - узлы (компьютеры), используемые в качестве серверов, имеющие свою собственную операционную систему.

**Узел (Node)** - компьютер, обычно включает в себя ЦП, память и дисковую подсистему.

#### Apache Hadoop

Apache Hadoop является программной платформой с открытым исходным кодом дял распредлеенного хранения и обработки больши объемов данных.

##### Особенности

- открытый исходный код

- распределенная файловая система HDFS

- неограниченные объемы данных

- HDFS является надстройкой над обычной файловой системой, например, такой, как Linux

- реализация парадигмы распределенных вычислений MapReduce

##### Виды аналитики, используемые в Hadoop

- текстовый анализ

- построение индексов

- построение и анализ графов 

- распознавание образов

- совместная фильтрация (Collaborative filtering)

- создание прогнозных моделей

- анализ тональности

- оценка рисков

#### Основные компоненты Apache Hadoop

##### Apache Hadoop Distributed File System (HDFS)

- архитектура "Master-Slave"

- построена на основе Google's File System (GFS)

- способна сохранять распределенные данные на разных узлах кластера

- репликация данных (данные копируются на несколько узлов)

- отказоустойчивость и высокая доступность

- поддержка MapReduce (распределнная и параллельная обработка)

- масштабируемость

### 2.2 Ключевые технологии и подходы

#### Ключевые определения HDFS

**Blocks** - файлы разбиваются на блоки (64 Mb по умолчанию), блоки хранятся в разных узлах.

**Replication Factor** - степень репликации каждого блока в файловой системе (по умолчанию - 3).

**NameNode (NN)** - сервис, который хранит (в памяти) метаданные обо всех файлах и дирректориях в файловой системе.

**Secondary NameNode** - периодически выполняет применение накопившихся изменений, сокращая время старта сервиса NameNode (сохраняет последнее состояние snapshot).

**DataNode (DN)** - хранит данные в блоках.

#### HDFS High Availibility (HDFS HA)

HDFS High Availibility (HA) - функция высокой доступности HDFS, запускаются два NameNode: Activity и Standby, позволяя быстро переключаться между ними в случае отказа одного.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-23-56-30-image.png" title="" alt="" data-align="center">

## Раздел 2. Ключевые технологии и подходы ч.2

### 2.3 Ключевые технологии и подходы

#### NameNode (NN)

**NameNode** хранит метаданные файловой системы

- информация о файле (имя, обновления, фактор репликации и т.д)

- информация о блоках и их расположения

- файл и его "отображение" на хранимые блоки

- права доступа к файлам

- количество файлов в кластере

- количество и статус DataNodes в кластере

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-09-58-51-image.png" title="" alt="" data-align="center">

#### DataNode (DN)

- обслуживает запросы на чтение запись от клиентов файловой системы

- выполняет запись блоков, удаление и репликацию на основе инструкций, полученных от NameNode

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-25-20-51-22-image.png" title="" alt="" data-align="center">

#### MapReduce

- программная платформа, позволяющая разрабатывать приложения, параллельно обрабатывающие большие объемы данных в кластере максимально надежным и отказоустойчивым способом.

- интегрирована с HDFS и предоставляет преимущества для параллельной обработки данных

- благодаря высокой степени абстракции проста для разработчиков

#### Преимущества MapReduce

**MapReduce обеспечивает**:

- автоматическую параллельную обработку и распределение больших объемов данных, хранящихся в кластере Hadoop

- отказоустойчивость

- планирование и контроль ввода-вывода

- мониторинг и контроль

#### MapReduce Job

**Задание состоит из:**

- входные данные (HDFS)

- исполнямой прогрммы MapReduce

**Hadoop выполняет задание, деля его на задачи:**

- Map tasks (user-definded)

- Shuffle and sort tasks (MapReduce)

- Reduce tasks (user-definded)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-10-24-46-image.png" title="" alt="" data-align="center">

#### MapReduce различия версий

**MapReduce 1 (MRv1 или классический MapReduce)**:

- используют  демоны JobTracker и TaskTracker

- выполняет только приложение MapReduce

**YARN (Yet Another Resource Negotiator или MRv2)**

YARN запускает нужный контейнер.

- использует демон REsourceManager (аналог NameNode)/NodeManager (аналог DataNode)

- выполняет приложения MapReduce, Impala, Spark и т.д.

#### Экосистема Hadoop

- Хранение:
  
  - **HDFS** - файловая система, предназначенная для хранения файлов больших размеров
  
  - **HBase** - column-oriented NoSQL база данных
  
  - **Solr** - платформа полнотекстового поиска (подсветка результатов, фасетный поиск)

- Обработка данных:
  
  - **Hive** - переводчик с HQL (язык похожий на SQL) в MapReduce
  
  - **Pig** - переводчик с PigLatin в MapReduce
  
  - **Oozie** - управляет последовательностью MapReduce работ (Workflow)
  
  - **Impala** - запросы в реальном времени к HDFS на HQL. Быстрее чем Hive в 5-40 раз

- Получение данных из разных источников:
  
  - **Flume** - сборщик данных в HDFS
  
  - **Sqoop** - обеспечивает взаимодействие Hadoop с RDBMS
  
  - **Nutch** - поисковый движок (краулер)

- Визуализация результатов
  
  - **Hue** - графический интерфейс для управления Hadoop кластером

**Поддержка:**

- Hive & Impala

- Pig

- HDFS

- Sqoop2

- YARN

- Oozie

- Solr

- HBase

- LDAP

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-10-38-14-image.png" title="" alt="" data-align="center">

## Раздел 3. Инструменты получения данных

### 3.1 Инструменты получения данных

* Напрямую в HDFS (CLI, WebHDFS, HttpFS, Hue)

* Fuse DFS (монтирование диска)

* SQOOP (структурированные данные)

* Flume (неструктурированные данные)

* Kafka (месенджер)

#### Напрямую в HDFS

##### Используя Hue, через Web brouser (не очень быстрый)

##### Используя командную строку:

```bash
hdfs dfs -help
hdfs dfs -put -p -f example.json /user/cloudera/example_tweets
```

**Плюсы:**

- масштабируемость

- не нужно промежуточное хранилище

**Минусы:**

- необходимо дополнительное ПО (Hadoop client)

##### Используя WebHDFS (web-client)

```bash
http://<active-namenode-server>:<namenode-port>/webhdfs/v1/<file-path>?op=OPEN
```

**Плюсы:**

* высокая производительность

* не требуется дополнителньое программное обеспечение

**Минусы:**

- сложный синтаксис

- не работает с HDFS HA

##### Используя HttpFS (web-client)

```bash
http://<hadoop-httpfs-server>:<httpfs-port>/webhdfs/v1/<file-path>?op=OPEN
```

**Плюсы:**

* не требуется дополнительное программное обеспечение

* работает с HDFS HA

**Минусы:**

- сложный синтаксис

- устанавливается как отдельный сервис

- представляет единую точку отказа

- производительность (все данные идут через 1 узел)

#### Fuse DFS

Позволяет получить доступ к HDFS, так, как будто это обычная локальная файловая система (монтируем HDFS)

1. Необходимо установить **hadoop-hdfs-fuse**

2. Создать папку монтирования и примонтировать HDFS
   
   ```bash
   mkdir -p <mount_point>
   hadoop-fuse-dfs dfs://<name_node_hostname>:<namenode_port><mount_point>
   ```

3. Записывать файлы, как в обычную папку

### 3.2 А что если данные лежат в базе данных?

#### Apache Sqoop (SQL to hadOOP)

- инструмент для обмена данными между Hadoop и Реляционными базами данных

- запускается из командной строки

- хорошо интегрирован с Hadoop Ecosystem (HDFS, Hbase, Hive, Oozie)

- использование MapReduce позволяет распараллелить процесс загрузки и сделать его эффективным

- интеграция с Hive позволяет создать исходные структуры 

#### Зачем нужен Sqoop?

- для получения структурированных данных **в Hadoop** (Import)
  
  ```bash
  sqoop import --connect jdbc:mysql://localhost/acmdb --
  table ORDERS --username test --password ****
  ```

- для передачи структурированных данных **из Hadoop** в реляционное хранилище данных (Export)
  
  ```bash
  sqoop export --connect jdbc:mysql://localhost/acmdb --
  table ORDERS_CLEAN --username test --password **** --
  export-dir /user/arvind/ORDERS
  ```

#### Ключевые особенности

- поддержка большого количества форматов (специальные форматы для sql-движков на hadoop):
  
  - CSV, AVRO, Sequence file, Parquet

- возможность загрузки в Hive (база данных) и Hbase

- интеграция в Oozie для создания расписаний

- поддержка специализированных коннектеров:
  
  - MySQL, PostgreSQL, Oracle, SQLServer, JDBC
  
  - Direct MySQL, Direct PostgreSQL, Direct Oracle

- коннекторы Direct - более производительные (Uses native tools)

- возможность наследовать или переопределить типы данных

- поддержка инкрементальной загрузки

#### Как это работает?

- данные разбиваются на части (partitions)

- Sqoop генерирует Java MapReduce код для обмена данными
  
  - тип данных определяется на основе метаданных

- Map часть передает данные

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-12-50-11-image.png" title="" alt="" data-align="center">

## Раздел 3. Инструменты получения данных ч.2

### 3.3 Apache Flume и Apache Kafka

Данные не всегда структурированы...

#### Apache Flume

- система сбора **разных** данных из **разных** источников

- надежный - транзакционная передача данных

- удобный - все описывается простой конфигурацией

- расширяемый - легко можно разработать свои компоненты

##### Компоненты Apache Flume

- **источник (source)**- отвечает за прием данных (EventDrivenSource, PollableSource)

- **канал (channel)** - компонент, выполняющий роль буфера при транспортировке данных

- **сток (sink)** - компонент, отвечающий за передачу данных на следующий этап обработки

- **агент (agent)** - процесс, в рамках которого функционируют компоненты Flume (источники, каналы, стоки)<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-12-54-41-image.png" title="" alt="" data-align="center">

##### Виды источников (source)

- Avro Source

- Thrift Source

- Exec Source

- Spooling Directory Source

- Taildir Source

- **Twitter 1% firehose Source**

- Kafka Source

- NetCat TCP/UDP Source

- Sequence Generator Source

- Syslog TCP/UDP Source

- **HTTP Source**

##### Виды каналов (channel)

- Memory Channel - хранение в памяти

- JDBC Channel - хранение в СУБД

- Kafka Channel - хранение в Apache Kafka

- File Channel - хранение в файле

- Spilable Memoty Channel - хранение в памяти с возможностью сброса на диск

- Custom Channel - возможность разработать свой канал 

##### Виды стоков (sink)

- HDFS Sink (стандартная HDFS hadoop)

- Hive Sink

- Logger Sink

- Avro Sink

- Thrift Sink

- IRC Sink

- File Roll Sink

- Null Sink

- Hbase Sink + Async

- MorphlineSolr Sink

- ElasticSearch Sink

- Kite Dataset Sink

- Kafka Sink

- HTTP Sink

- Custom sink

##### Виды потоков

###### Несколько агентов

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-23-07-image.png" title="" alt="" data-align="center">

###### Мультиплексирование

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-23-56-image.png" title="" alt="" data-align="center">

###### Объединение данных

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-24-33-image.png" title="" alt="" data-align="center">

##### Как настроить Flume

1. Создать конфигурационный файл (flume.conf)

2. Сохранить файл в папку flume-ng/conf

3. Настроить отдельные компоненты

4. Опционально: настроить параметры окружения flume-env.sh

5. Проверить настройку через команду `flume-ng help`

##### Конфигурирование

```
# заголовочная часть
TwitterAgent.sources = Twitter
TwitterAgent.channels = MemChannel
TwitterAgent.sinks = HDFS

# SOURCE TWITTER
TwitterAgent.sources.Twitter.type = com.cloudera.flume.source.TwitterSource
TwitterAgent.sources.Twitter.channels = MemChannel
TwitterAgent.sources.Twitter.consumerKey =
TwitterAgent.sources.Twitter.consumerSecret =
TwitterAgent.sources.Twitter.accessToken =
TwitterAgent.sources.Twitter.accessTokenSecret =
TwitterAgent.sources.Twitter.keywords = russia, moscow, россия, москва

# CHANNEL
TwitterAgent.channels.MemChannel.type = momory
TwitterAgent.channels.MemChannel.capacity = 100
TwitterAgent.channels.MemChannel.transactionCapacity = 100

# SINK HDFS
TwitterAgent.sinks.HDFS.channel = MemChannel
TwitterAgent.sinks.HDFS.type = hdfs
TwitterAgent.sinks.HDFS.hdfs.path = hdfs://user/cloudera/Tweets/russia
TwitterAgent.sinks.HDFS.hdfs.fileType = DataStream
TwitterAgent.sinks.HDFS.hdfs.writeFormat = Text
TwitterAgent.sinks.HDFS.hdfs.batchSize = 10
TwitterAgent.sinks.HDFS.hdfs.rollSize = 0
TwitterAgent.sinks.HDFS.hdfs.rollInterval = 600
TwitterAgent.sinks.HDFS.hdfs.rollCount = 10000
```

#### Apache Kafka

- распределенная система для обмена сообщениями, разработанная в рамках фонда Apache компанией LinkedIn

- работает согласно принципу "публикация - подписка"

- высокая скорость работы для публикации и подписки (более 500 миллиардов событий в день)

- масштабируемость

- потребители могут регулировать скорость получения данных и запрашивать сообщения повторно

##### Ключевые особенности

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-44-53-image.png" title="" alt="" data-align="center">

##### Архитектура

- producers (приложения, которые пишут данные в канал)

- topic (куда producers пишут данные)

- consumers (приложения, которые читают данные из topic)<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-45-22-image.png" title="" alt="" data-align="center">

##### Как это работает?

- данные организованы в топиках (topic)

- producer - пишет любые данные в топик

- consumer - читает данные из топика (сам решает с какого момента)

- к сообщениям можно получить доступ по смещению (offset)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-48-24-image.png" title="" alt="" data-align="center">

##### Kafka Message

Состоит из:

- **Offset** - номер сообщения в очереди топика

- **Key** - позволяет определить в какую партицию попадает сообщение (Routing)

- **Value** - набор байт (само сообщение)

##### Как это работает

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-13-52-04-image.png" title="" alt="" data-align="center">

* каждый топик состоит из партиций

* партиция это файл на диске

* партиции реплицируются (несколько партиций для одного топика)

* количество реплик настраивается

* у реплик есть Leader и Follower

* данные хранятся настраиваемое время (не удаляется сразу после прочтения)

* запись всегда в конце файла

##### Примеры

- **Создать топик**

```
kafka-topics --create --zookeeper localhost:2181/
kafka --replication-factor 1 --partitions 1 --
topic test
```

* **Посмотреть топики**

```
kafka-topics --list --zookeeper localhost:2181/
kafka
```

- **Послать сообщение**

```
kafka-console-producer --broker-list localhost:
9092 --topic test 
```

* **Прочесть сообщение**

```
kafka-console-consumer --zookeeper localhost:2181/
kafka --topic test
```

## Раздел 4. Обработка данных

### 4.1 Обработка данных в Apache Spark

#### Apache Spark

- это фреймворк параллельной обработки данных с открытым исходным кодом с доказанной масштабируемостью до 2000 узлов

- дополняет Apache Hadoop

- упрощает разработку быстрых, унифицированных приложений Big Dat, комбинируя пакетную, потоковую и интерактивную аналитику всех анализируемых данных

- может работать на кластере (YARN, Mesos) и полностью автономно

#### Apache Spark Ecosystem

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-14-07-35-image.png" title="" alt="" data-align="center">

#### Apache Spark vs Hadoop MapReduce

**Apache Spark:**

- хранит данные в памяти

- может обрабатывать потоки данных (streaming), порции данных (batch) и использовать библиотеки машинного обучения

- написан на Scala

- выполняет аналогичные задачи в 10-100 раз быстрее, чем MapReduce, за счет использования большого количества оперативный памяти

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-14-08-04-image.png" title="" alt="" data-align="center">

**Hadoop MapReduce:**

- файлы разбиваются на блоки, блоки хранятся на разных узлах

- в основном используется для построения отчетов на основе исторических данных; это обработка порций данных (batch)

- написана на Java

- не утилизирует оперативную память кластера на максимум

#### Как можно использовать Spark? Для чего?

- обработка данных в реальном времени
  
  - выявление мошеннических схем
  
  - отслеживание активности не веб-сайтах
  
  - отслеживание активности в сетях предприятия
  
  - рекомендателньые системы

- сегментация пользователей для маркетинговых компаний

- анализ отношения к тематике

- машинное обучение

#### Из чего состоит Spark?

**Spark Core** - основа фреймворка, отвечает за:

- управление памятью и восстановление после отказов

- планирование, распределение и отслеживание заданий кластере

- взаимодействие с ситемами хранения данных

**Spark SQL** - библиотека фреймворка для структурированной обработки данных. Она использует DataFrames и может выполнять запросы SQL в распределенном режиме. Это позволяет выполнять запросы Hive (база данных) до 100 раз быстрее.

**Spark Streaming** - простой в использовании инструмент обработки потоковых данных. Spark Streaming не обрабатывает данные в реальном времени, а делает это в режиме miicro-batch. "Под капотом" Spark Streaming получает входные потоки данных и разбивает данные на пакеты. Далее они обрабатываются движком Spark, после чего генерируется конечный поток данных. Structured Streaming (добавлен в Spark 2.x) - позволяет работать с бесконечным DataFrames, в который добавляются потоковые данные.

**MLlib** - это библиотека дя машинного обучения, предоставляющая различные алгоритмы, разработанные для горизонтального масштабирования на кластере. Некоторые из этих алгоритмов работают и с потоковыми данными - например, линейная регрессия с использованием обычного метода наименьших квадратов или кластеризация по методу k-средних.

**MLlib включает в себя популярные алгоритмы:**

- классификация

- регрессия

- деревья принятия решений

- рекомендация

- кластеризация

- тематическое моделирование

**GraphX** - это библиотека для масштабируемой обработки графовых данных. Дополнительная библиотека GraphFrames позволяет выполнять графовые алгоритмы над структурированными объектами используя DataFrames.

#### Ключевые особенности Spark

- Java, Scala, Python, R

- быстрее чем MapRaduce (до 100 раз)

- работает с Resilient Distributed Dataset (RDD)
  
  - трансформации (новый RDD)
  
  - действия (вывод информации)

- все операции выполняются в памяти

- активное развития

- интеграция с надстройками

- может работать на кластере и отдельно (удобно для разработки)

### 4.2 Обработка данных

* Resilient Distributed Dataset (RDD)
  
  - трансформации (новый RDD)
  
  - действия (вывод информации, подсчет)

#### Архитектура

- Spark Driver (точка входа, создает SparkContext)

- Spark Master

- Cluster Manager (не всегда принадлежит spark)

- Executurs (выполняют задачи)

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-14-36-05-image.png" title="" alt="" data-align="center">

#### Spark Driver

- запускается в контейнере Application Master (если работает в YARN)

- создает **SparkContext** - экземпляр соединения с Spark Master и Spark Worker

- планирует, запускает по расписанию и отслеживает выполнение Spark приложения

- контролирует выполнение вашшей задачи ил изапроса

- собирает и возращает клиенту результаты

- но основе всех трансформаций и действий создает направленный ациклический граф (DAG)

#### Система обработки Directed Acyclic Graph

Spark использует Directed Acyclic Graph (DAG) - некий сценарий, который нужно выполнить дял получения результата.

##### Основные свойства DAG:

- направленный (Directed) - проходит только в одном направлении

- нецикличный (Acyclic) - нет циклов и возвратов

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-28-20-41-44-image.png" title="" alt="" data-align="center">

##### DAG состоит из tasks и stages:

- **Task**- самый гранулированный элемент запускаемой работы

- **Stage** - набор tasks, которые запускаются вместе. Между Stage есть зависимости (одна идет за другой)

- на основе всех трансформаци и действий создает **направленный ациклический граф (DAG)**

#### Spark Master

* **Spark Master и Cluster Manager** - главный процесс, который отслеживает, резервирует и выделяет ресурсы кластреа (в случа с YARN - контейнеры), необходимые для работы.

* **Spark Master и Cluster Manager** могут быть отдельными процессами (в случае с YARN) ибо могут быть одним процессом (Spark Standalone)

* Spark Master запрашивает ресурсы у кластера и по готовности оповещает об этом Spark Driver

* в любом режиме развертывания (кластер, самостоятельно), мастер соотносит выделенные ресурсы или контейнеры с рабочими узлами (Worker Nodes) и отслуживает их состояние и прогресс выполнения

#### Cluster Manager

В настоящий момент Spark поддерживает 3 Cluster Manager:

* **Standalone** - процесс Spark Mаster выступает простым cluster manager

* **YARN (ResourceManager)** - можно запускть Spark приложения в двух режимах:
  
  - **Yarn-client** (Driver будет запущен на клиентской машине)
  
  - **Yarn-cluster** (Driver будет запущен на каком-то узле кластера)

* **Apache Mesos**
  
  - общий инструмент управления кластером, который также поддерживает запуск MapReduce приложении
  
  - не входит в состав дистрибутивов Hadoop

#### Executors

- **Spark executors** - процессы на узлах, которые запускает задачи (tasks) из Spark DAG.

- Executor разервирует на узлах CPU и RAM для выполнения задач

- Executors относятся к конретному Spark приложению и освобождают ресурсы когда завершится приложение

#### Resilient Distributed Dataset (RDD)

- **Resilient** - если сломается узел, который выполняет часть Spark приложения, набор данных, который хранился на узле может быть восстановлен на другом узле

- **Distributed** - данные в RDD разделены на несколько партиций и распределены между узлами кластера

### 4.3 Обработка данных

#### Resilient Distributed Dataset (RDD), продолжение

- **Data set:**
  
  - RDD - набор данных, который состоит из записей (строк).
  
  - Записи представляют собой уникально идентифицируемые коллекции в наборе данных.
  
  - Записи могут представлять собой набор полей, похожих на строки в таблице, строку текста в файле или некоторые сложные объекты.

#### Терминология RDD

RDD - главная абстракция spark.

Партиции - единица параллелизма, чем больше партиций, тем больше узлов мы можем задействовать для обработки этого набора данных.

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-25-15-image.png" title="" alt="" data-align="center">

#### Операции RDD

##### Трансформации (повлекут создание нового набора данных):

- Связывание (Map)

- Фильтрация (Filter)

- Выборки (Sample)

- Объединение (Union)

- Группирование по ключу (Groupbykey)

- Свертка по ключу (Reducebykey)

- Соединение и кеширование (Join & cache)

Трансформации в Spark осуществляются в "ленивом" режиме - результат не вычисляется сразу после трансформации. Вместо этого они "запоминают" операцию, которую следует произвести, и набор данных, над которым нужно совершить операцию. Вычисление трансформаций происходит только тогда, когда вызывается действие.

##### Действия (повлекут какой-то результат):

- Свертка (Reduce)

- Сбор (Collect), например, показать строки

- Подсчет (Count), например, количества строк в наборе

- Сохранение (Save)

- Поиск (Lookpkey)

#### Пример RDD

![](/home/vibo/Pictures/GlobalMarkText/2022-09-22-22-25-46-image.png)

#### WordCount on PySpark

- Читаем файл

```python
text_file = sc.textFile("hdfs://...")
```

- Делим по пробелам, присваиваем каждому слову 1, группируем по словам и складываем числа

```python
counts = text_file.flatMap(lambda line: line.split(" ")) \
              .map(lambda word: (word, 1)) \
              .reduceByKey(lambda a,b: a + b)
```

* Сохраняем

```python
counts.saveAsTextFile("hdfs://...")
```

### 4.4 Обработка данных

#### Spark Streaming

Модуль **Spark Streaming** работает с потоковыми данными. Spark разбивает потоки данных на RDD и позволяет выполнять те же действия, что и с обычными данными.

- Простота использования (пишем потоковые программы не сильно меняя код)

- Отказоустойчивость (перезапускает приложение на другом узле, если упало)

- Унификация (streaming, batch, interactive)

- Поддержка машинного обучения и SQL

- Оперирует Dstream - Discretized stream <img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-19-50-image.png" title="" alt="" data-align="center">

#### WordCount

Смотрим, как запустить предыдущий пример в стриминге и постоянно высчитывать количество слов, которые идут в нашем потоке.

```python
pairs = words.map(lambda word: (word, 1))
# считаем сколько слов в каждой порции
wordCounts = pairs.reduceByKey(lambda x, y: x + y)
# выводим информацию на экран
wordCounts.pprint()
# запуск вычислений
ssc.start()
# ждем завершения
ssc.awaitTermination()
```

#### DStream

- Dstream - поток данных RDD, основной уровень абстракции для Spark Streaming

- Преимущества:
  
  - балансировка нагрузки (можно распределять данные между разными узлами)
  
  - быстрое востановление после сбоев (если какой-то сервер упалЮ то все данные, которые на него шли распределяются между другими серверами)
  
  - унификация (код, который писали для обработки набора данных, батча - можно использовать в стриминге)

##### Балансировка нагрузки

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-22-22-15-59-image.png" title="" alt="" data-align="left">

##### Быстрое восстановление после сбоев

![](/home/vibo/Pictures/GlobalMarkText/2022-09-22-22-16-29-image.png)

### 4.5 Apache Hive

**Проблема:**

- MapReduce код пишется на Java, что не всегда удобно

- нужен человек, знающий Java, разбирающийся в MapRaduce

**Решение:**

- SQL - знают очень многие!

- решение от Facebook

- HiveQL - язык запросов похож на SQL

```sql
SELECT product.name, SUM(orders.sales_price)
    FROM product INNER JOIN orders
ON (product.id = orders.product_id) WHERE
    orders.year = '2012'
GROUP BY product.name;
```

#### Ключевые особенности

- проект с открытым исходным кодом, разработан в Facebook

- позволяет использовать SQL для обработки и получения данных из HDFS используя язык HiveQL (очень похож на SQL)

- **Hive** - инфраструктура поверх Hadoop для анализа больших наборов данных

- Hive преобразовывает HiveQL запросы в:
  
  - MapReduce программы
  
  - Spark программы

#### Как данные хранятся в HDFS?

- данные хранятся в файлах и папках
  
  - любые типы файлов
  
  - тройная репликация по всему кластеру

- схема на чтение
  
  - каждый инструмент самостоятельно интерпретирует данные

#### Структурирование данных с Hive

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-19-41-50-image.png" title="" alt="" data-align="center">

Таблица включает в себя информацию о том, как разобрать данные используя Java:

- InputFormat показывает как разбить файл на куски (Splits)

- RacordReader показывает как разбить кусок на строки

- SerDe на основе строки создает колонки

#### Как Hive читает Любые! данные?

<img title="" src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-19-44-50-image.png" alt="" data-align="center">

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-19-45-46-image.png" title="" alt="" data-align="center">

#### Создание таблиц над файлами в HDFS

```
{"custid":118692, "movieid":null, "genreid":null, "time":"2012-07-01:00:00:07", "recommended":null, "activity":8}
{"custid":1374234, "movieid":1948, "genreid":9, "time":"2012-07-01:00:00:22", "recommended":"N", "activity":7}
```

**HiveQL**

```
CREATE EXTERNAL TABLE default.example_json(
    custid int ,
    movied int ,
    genreid int ,
    time string ,
    recommended string ,
    activity int ,
    rating int ,
    price float ,
    position int )
ROW FORMAT SERDE
    'org.apache.hive.hcatalog.data.JsonSerDe'
STORED AS INPUTFORMAT
    'org.apache.hadoop.mapred.TextInputFormat'
OUTPUTFORMAT
    'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'
LOCATION
    'hdfs://somehost.localdomain:8020/user/bigdata/somedata_json'
```

#### Основные объекты Hive

- Databases

- Tables

- Partitions

#### Hive Metastore

- содержит информацию о базах данных, таблицах, партициях

- информацию о том, как читать данные из HDFS

- представляет собой реляционную базу данных

#### Архитектура

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-20-05-47-image.png" title="" alt="" data-align="center">

#### Применение Hive

- обработка логов

- обработка текстов

- индексирование документов

- бизнес аналитика

#### Ограничения Hive

- нет поддержки materrialized views

- слабая поддержка транзакций

- медленная работа для легких запросов

- ограничения в поддержки подзапросов

- нет полной поддержки SQL-92

- незрелый оптимизатор

### 4.6 Cloudera Impala

#### Impala

**Impala** - движок с массово параллельной обработкой данных для языка SQL (MPP)

- почти Real-time

- не переводит SQL в MapReduce

- использует своих демонов для выполнения SQL параллельно

- возможность запроса к данным в HDFS и HBase

- ANSI-92 SQL + пользовательские функции (UDFs)

- поддержка форматов: text, SequenceFiles, Avro, RCFile, LZO и Parquet

#### Поддерживаемые форматы

- Text и SequenceFiles, которые могут быть сжаты:
  
  - Snappy
  
  - GZIP
  
  - BZIP

- Avro

- RCFile

- LZO text file

- ANSI-92 SQL + пользовательские функции (UDFs)

- Parquet

#### Доступ к Impala

Запуск SQL запросов возможен через:

- Impala-shell - командная строка

- Hue - веб интерфейc

- JDBC и ODBC интерфейсы

#### Как Impala взаимодействует с Hive?

- использует существующую Hive инфраструктуру

- хранит описание своих таблиц в Hive Metastore

- имеет доступ к Hive таблицам

- Impala - быстрые запросы

- использует `COMPUTE STATS` для подсчета статистики

#### Как Impala взаимодействует с HDFS и HBase

- HDFS
  
  - основная система хранения данных дял Impala
  
  - данные хранятся в файлах

- HBase
  
  - альтернативная система хранения данных
  
  - таблицы в Impala могут запрашивать данные из таблиц HBase (Mapped)

#### Основные преимущества Impala

- производительность MPP (свой собственный движок)

- не надо перемещать данные в реляционную базу данных (для простых запросов)

- позволяет проводить анализ сырых и исторических данных

- безопасность (хорошо интегрируется с системой безопасности Hadoop)

## Раздел 5. Хранение данных

### 5.1 Хранение данных

#### Apache HBase

**HBase** - column-oriented NoSQL база данных основанная Google BigTable

- масштабируемость и отказоустойчивость

- открытый исходный код (Java)

- работает с петабайтами данных

- миллионы добавлений в секунду

- данные хранятся в HDFS

- всего 3 операции: get/put/scan

- имеет автоматические временные срезу

- не предоставляет SQL-доступ

#### Базовые понятия

- данные хранятся в таблицах (table)

- таблица содержит строки (row)
  
  - доступ к строке по уникальному ключу (RowKey) массив байт
  
  - строки отсортированы

- строки группируются по колонкам в **column families** (Cf, Family)

- данные хранятся в ячейках (cells)

- данные - байтовый массив, хранить можно что угодно

#### Column Family (CF)

**Column Family** - способ организации данных, которые часто используются совместно.

- к Column Family применяется сжатие

- хранятся в одном или нескольких файлах (HFile / StoreFile)

- именования и количество CF (Column Family) задается при создании таблицы

- CF содержит в себе колонки

- колонок может не быть произвольное количество, можно добавлять новые колонки в процессе работы с базой

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-21-18-28-image.png" title="" alt="" data-align="center">

#### Версионированность

- каждая CF (Column Family) может иметь одну и более версий

- значения в колонках дополнительно содержат время записи (timestamp) или версию

- именования и количество CF (Column Family) задается при создании таблицы

- при записи и чтении значения можно явно указать какую версию прочесть. По умолчанию читается последняя версия, а записывется новая с текущем временем

**Value = Table + RowKey + ColunmFamily + ColumnName + Timestamp**

#### Доступ к данным

- Hbase shell (командная строка, базовый инструмент)

- Java API (соответствующие библиотеки)

- REST Server

- Thrift

- доступ по ключу или набору ключей (наиболее правильный подход)

- сканирование
  
  - с фильтрами по: rows, ColumnFamily, Timestamp, ColumnName, Value
  
  - фильтрация будет происходить на каждом RegionServer

#### Стоит использовать HBase?

**ДА**

- большой объем данных

- доступ к данным
  
  - по ключю
  
  - последовательный доступ по диапазону ключей

- необходима свободная схема данных
  
  - разная структура для разных строк

**НЕТ**

- нужны транзакции (в HBase этого нет)

- нужна аналитика (HBase нет)

- нужно объединение данных: join, group by, sort (HBase не поддерживает)

- нужен SQL

## Раздел 5. Хранение данных ч.2

### 5.2 Apache Solr

- в дитсрибутиве Cloudera называется **Cloudera Search** - обеспечивает поиск по данным, можно создать внутренний поисковик по данным в организации

- Обеспечивает:
  
  - анализ различных типов данных
  
  - упрощенное взаимодействие с другими сервисами Hadoop
  
  - масштабируемость, гибкость и надежность поисковых сервисов

- поисковый индекс харнится в HDFS

- индексирование большого объема данных через MapReduce

- индексирование в режиме реального времени, масштабируемое индексирование данных

- поиск по данным HBase

#### Особенности

- Rest API

- построен на библиотеке Lucene (отвечает за построение индексов)

- Open Source

- поиск по данным HBase

- масштабируемый

- отказоустойчивый

- текстовый поиск быстрее чем в базе данных

- текстовая обработка разных языков

#### Как это работает?

- данные загружаются в Solr в различных форматах (Text, Word, Xml, Pdf, Json)

- Solr индексирует данные (разбирает их) используя файл schema.xml

- в файле schema.xml есть описане процесса разбора файла

- запросы к Solr позволяют искать данные по условиям, как в классических поисковиках (goolge, yandex, yahoo)

#### Что умеет Solr?

- очень быстрые поисковые запросы

- очень быстрое получение данных

- группировка данных, фасетный поиск

- поиск по гео-данным, ограниченная область поиска

- поддержка autocomplete

- подсвечивание результатов

- аналитика по данным

#### Как можно использовать Solr?

- поисковый движок для больших сайтов

- аналитика

- ускорение доступа к отдельным данным/запросам

- рекомендации

- Cloudera Searchизучение данных

#### Архитектура и терминология

<img src="file:///home/vibo/Pictures/GlobalMarkText/2022-09-23-23-17-10-image.png" title="" alt="" data-align="center">

- **Node** - узел, на котором запущен Solr

- **Core** = индекс (проиндексированные документы, их часть) + схема (файл, в котором написано как файлы были проиндексированы) + конфигурация solrconfig.xml (конфигурация инстанска, как он по этому индексу отдает данные)

- **Schema** - описывет метаданные о документах, типы данных, как разбирать документы, как хранить

- **ConfigSet**- набор конфигураций, доступных нескольним Core

- **SolrCloud** - кластер из нескольких **Solr Nodes**
  
  - **collection** - логический индекс, разбитый на несколько узлов
  
  - **shard** - часть collection
  
  - replica/leader - состояние shard
  
  - zookeeper - распределенный сервис хранения конфигураций и синхронизации, необходим для SolrCloud

#### Базовые понятия

- у каждого Solr Core есть схема, описываемая в файле schema.xml

- содержит:
  
  - поля,
  
  - типы данных каждого поля (int, float, long, doble, date, string, text (много языков), location
  
  - анализаторы
  
  - атрибуты полей: indexed, stored, type, multivalued

Пример: 

```
<field name="id"type="string" indexed="true" stored="true" required="true" multivalued="false" />
```

#### Пример

```
curl -X Post -H 'Content-Type: application/json'
'http://localhost:8983/solr/my_collection/update' -data-
binary
'[
{
"id":"1"
},
{
"id":"2",
"title":"Doc 2"
}
]
```

#### Индексирование

**Индексировать документы можно через:**

- Post команду

- REST API

- SolrJ и другие библиотеки

- DataImportHandler

#### Пример

```
curl -X Post -H 'Content-Type: application/json'
'http://localhost:8983/solr/my_collection/update' -data-
binary
'[
{
"id":"1",
"title":"Doc 1"
},
{
"id":"2",
"title":"Doc 2"
}
]
```

#### Поиск

* поддержка интервала [ ]  и { }
  
  - price: [0 to 100 }

* поддержка поиска по дате: 2018-10-16Т20:20:11z

* bolean операторы
  
  - +яблоко -груша
  
  - яблоко AND груша

* поиск по полям
  
  - Title: Book123
- четкие и нечеткие условия поиска (Regexp)
  
  - Pro*
  
  - J?t
  
  - /H.*t/

- сортировка результатов
  
  - &sort=name asc, age desk

# 
